{
  "raw_text": "Deep Residual Learning for Image Recognition\nKaimingHe XiangyuZhang ShaoqingRen JianSun\nMicrosoftResearch\nkahe,v-xiangz,v-shren,jiansun @microsoft.com\n{ }\nAbstract 20\nDeeper neural networks are more difficult to train. We\npresentaresiduallearningframeworktoeasethetraining 10\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ningresidualfunctionswithreferencetothelayerinputs,in- 0 0 1 2 iter. 3 (1e4) 4 5 6\nsteadoflearningunreferencedfunctions. Weprovidecom-\nprehensive empirical evidence showing that these residual\nnetworksareeasiertooptimize,andcangainaccuracyfrom\nconsiderablyincreaseddepth. OntheImageNetdatasetwe\nevaluateresidualnetswithadepthofupto152layers\u20148\n\u00d7 deeperthanVGGnets[41]butstillhavinglowercomplex-\nity.Anensembleoftheseresidualnetsachieves3.57%error\nontheImageNettestset.Thisresultwonthe1stplaceonthe\nILSVRC2015classificationtask. Wealsopresentanalysis\nonCIFAR-10with100and1000layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremelydeeprepresentations,weobtaina28%relativeim-\nprovement on the COCO object detection dataset. Deep\nresidualnetsarefoundationsofoursubmissionstoILSVRC\n& COCO 2015 competitions1, where we also won the 1st\nplacesonthetasksofImageNetdetection,ImageNetlocal-\nization,COCOdetection,andCOCOsegmentation.\n1.Introduction\nDeep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classification [21,\n50, 40]. Deep networks naturally integrate low/mid/high-\nlevel features [50] and classifiers in an end-to-end multi-\nlayer fashion, and the \u201clevels\u201d of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[41,44]revealsthatnetworkdepthisofcrucialimportance,\nand the leading results [41, 44, 13, 16] on the challenging\nImageNetdataset[36]allexploit\u201cverydeep\u201d[41]models,\nwithadepthofsixteen[41]tothirty[16]. Manyothernon-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/ and\nhttp://mscoco.org/dataset/#detections-challenge2015.\n)%(\nrorre\ngniniart\n20\n10\n00 1 2 3 4 5 6 iter. (1e4)\n)%(\nrorre\ntset\n56-layer\n20-layer\n56-layer\n20-layer\nFigure1.Trainingerror(left)andtesterror(right)onCIFAR-10\nwith20-layerand56-layer\u201cplain\u201dnetworks. Thedeepernetwork\nhashighertrainingerror,andthustesterror. Similarphenomena\nonImageNetispresentedinFig.4.\ngreatlybenefitedfromverydeepmodels.\nDrivenbythesignificanceofdepth,aquestionarises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [1, 9], which\nhamper convergence from the beginning. This problem,\nhowever,hasbeenlargelyaddressedbynormalizedinitial-\nization[23,9,37,13]andintermediatenormalizationlayers\n[16],whichenablenetworkswithtensoflayerstostartcon-\nverging for stochastic gradient descent (SGD) with back-\npropagation[22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly. Unexpectedly,\nsuch degradation is not caused by overfitting, and adding\nmorelayerstoasuitablydeepmodelleadstohighertrain-\ningerror,asreportedin[11,42]andthoroughlyverifiedby\nourexperiments. Fig.1showsatypicalexample.\nThedegradation(oftrainingaccuracy)indicatesthatnot\nallsystemsaresimilarlyeasytooptimize. Letusconsidera\nshallower architecture and its deeper counterpart that adds\nmorelayersontoit. Thereexistsasolutionbyconstruction\ntothedeepermodel: theaddedlayersareidentitymapping,\nand the other layers are copied from the learned shallower\nmodel. Theexistenceofthisconstructedsolutionindicates\nthatadeepermodelshouldproducenohighertrainingerror\nthan its shallower counterpart. But experiments show that\nourcurrentsolversonhandareunabletofindsolutionsthat\n1\n5102\nceD\n01\n]VC.sc[\n1v58330.2151:viXra\nImageNet test set, and won the 1st place in the ILSVRC\nx\n2015 classification competition. The extremely deep rep-\nweight layer\nresentationsalsohaveexcellentgeneralizationperformance\nF (x) relu x on other recognition tasks, and lead us to further win the\nweight layer\nidentity 1st places on: ImageNet detection, ImageNet localization,\nCOCO detection, and COCO segmentation in ILSVRC &\n(x)(cid:1)+(cid:1)x\nF relu COCO2015competitions. Thisstrongevidenceshowsthat\nFigure2.Residuallearning:abuildingblock. theresiduallearningprincipleisgeneric,andweexpectthat\nitisapplicableinothervisionandnon-visionproblems.\narecomparablygoodorbetterthantheconstructedsolution\n(orunabletodosoinfeasibletime). 2.RelatedWork\nIn this paper, we address the degradation problem by\nintroducing a deep residual learning framework. In- Residual Representations. In image recognition, VLAD\nstead of hoping each few stacked layers directly fit a [18]isarepresentationthatencodesbytheresidualvectors\ndesired underlying mapping, we explicitly let these lay- with respect to a dictionary, and Fisher Vector [30] can be\ners fit a residual mapping. Formally, denoting the desired formulated as a probabilistic version [18] of VLAD. Both\nunderlyingmappingas (x), weletthestackednonlinear ofthemarepowerfulshallowrepresentationsforimagere-\nlayersfitanothermappin H gof (x):= (x) x. Theorig- trieval and classification [4, 48]. For vector quantization,\ninalmappingisrecastinto ( F x)+x.W H ehyp \u2212 othesizethatit encoding residual vectors [17] is shown to be more effec-\niseasiertooptimizethere F sidualmappingthantooptimize tivethanencodingoriginalvectors.\nthe original, unreferenced mapping. To the extreme, if an In low-level vision and computer graphics, for solv-\nidentity mapping were optimal, it would be easier to push ing Partial Differential Equations (PDEs), the widely used\ntheresidualtozerothantofitanidentitymappingbyastack Multigrid method [3] reformulates the system as subprob-\nofnonlinearlayers. lems at multiple scales, where each subproblem is respon-\nsiblefortheresidualsolutionbetweenacoarserandafiner\nTheformulationof (x)+xcanberealizedbyfeedfor-\nF scale. AnalternativetoMultigridishierarchicalbasispre-\nwardneuralnetworkswith\u201cshortcutconnections\u201d(Fig.2).\nconditioning[45,46], whichreliesonvariablesthatrepre-\nShortcut connections [2, 34, 49] are those skipping one or\nsentresidualvectorsbetweentwoscales. Ithasbeenshown\nmore layers. In our case, the shortcut connections simply\n[3,45,46]thatthesesolversconvergemuchfasterthanstan-\nperform identity mapping, and their outputs are added to\ndard solvers that are unaware of the residual nature of the\nthe outputs of the stacked layers (Fig. 2). Identity short-\nsolutions.Thesemethodssuggestthatagoodreformulation\ncut connections add neither extra parameter nor computa-\norpreconditioningcansimplifytheoptimization.\ntional complexity. The entire network can still be trained\nend-to-endbySGDwithbackpropagation,andcanbeeas-\nShortcut Connections. Practices and theories that lead to\nily implemented using common libraries (e.g., Caffe [19])\nshortcutconnections[2,34,49]havebeenstudiedforalong\nwithoutmodifyingthesolvers.\ntime. Anearlypracticeoftrainingmulti-layerperceptrons\nWe present comprehensive experiments on ImageNet (MLPs)istoaddalinearlayerconnectedfromthenetwork\n[36] to show the degradation problem and evaluate our input to the output [34, 49]. In [44, 24], a few interme-\nmethod. Weshowthat: 1)Ourextremelydeepresidualnets diate layers are directly connected to auxiliary classifiers\nare easy to optimize, but the counterpart \u201cplain\u201d nets (that for addressing vanishing/exploding gradients. The papers\nsimply stack layers) exhibit higher training error when the of[39,38,31,47]proposemethodsforcenteringlayerre-\ndepthincreases;2)Ourdeepresidualnetscaneasilyenjoy sponses,gradients,andpropagatederrors,implementedby\naccuracygainsfromgreatlyincreaseddepth,producingre- shortcutconnections. In[44],an\u201cinception\u201dlayeriscom-\nsultssubstantiallybetterthanpreviousnetworks. posedofashortcutbranchandafewdeeperbranches.\nSimilarphenomenaarealsoshownontheCIFAR-10set Concurrentwithourwork,\u201chighwaynetworks\u201d[42,43]\n[20], suggesting that the optimization difficulties and the present shortcut connections with gating functions [15].\neffectsofourmethodarenotjustakintoaparticulardataset. These gates are data-dependent and have parameters, in\nWepresentsuccessfullytrainedmodelsonthisdatasetwith contrast to our identity shortcuts that are parameter-free.\nover100layers,andexploremodelswithover1000layers. When a gated shortcut is \u201cclosed\u201d (approaching zero), the\nOn the ImageNet classification dataset [36], we obtain layers in highway networks represent non-residual func-\nexcellentresultsbyextremelydeepresidualnets. Our152- tions. On the contrary, our formulation always learns\nlayerresidualnetisthedeepestnetworkeverpresentedon residual functions; our identity shortcuts are never closed,\nImageNet, while still having lower complexity than VGG and all information is always passed through, with addi-\nnets [41]. Our ensemble has 3.57% top-5 error on the tional residual functions to be learned. In addition, high-\n2\nway networks have not demonstrated accuracy gains with ReLU [29] and the biases are omitted for simplifying no-\nextremelyincreaseddepth(e.g.,over100layers). tations. The operation + x is performed by a shortcut\nF\nconnection and element-wise addition. We adopt the sec-\n3.DeepResidualLearning ondnonlinearityaftertheaddition(i.e.,\u03c3(y),seeFig.2).\nTheshortcutconnectionsinEqn.(1)introduceneitherex-\n3.1.ResidualLearning\ntraparameternorcomputationcomplexity. Thisisnotonly\nLet us consider (x) as an underlying mapping to be attractiveinpracticebutalsoimportantinourcomparisons\nH\nfit by a few stacked layers (not necessarily the entire net), between plain and residual networks. We can fairly com-\nwithxdenotingtheinputstothefirstoftheselayers. Ifone pare plain/residual networks that simultaneously have the\nhypothesizes that multiple nonlinear layers can asymptoti- same number of parameters, depth, width, and computa-\ncallyapproximatecomplicatedfunctions2,thenitisequiv- tionalcost(exceptforthenegligibleelement-wiseaddition).\nalent to hypothesize that they can asymptotically approxi- The dimensions of x and must be equal in Eqn.(1).\nF\nmate the residual functions, i.e., (x) x (assuming that Ifthisisnotthecase(e.g.,whenchangingtheinput/output\nH \u2212\nthe input and output are of the same dimensions). So channels), we can perform a linear projection W s by the\nratherthanexpectstackedlayerstoapproximate (x),we shortcutconnectionstomatchthedimensions:\nH\nexplicitly let these layers approximate a residual function\n(x) := (x) x. The original function thus becomes y= (x, W i )+W s x. (2)\nF H \u2212 F { }\n(x)+x.Althoughbothformsshouldbeabletoasymptot-\nF WecanalsouseasquarematrixW inEqn.(1). Butwewill\nicallyapproximatethedesiredfunctions(ashypothesized), s\nshowbyexperimentsthattheidentitymappingissufficient\ntheeaseoflearningmightbedifferent.\nforaddressingthedegradationproblemandiseconomical,\nThis reformulation is motivated by the counterintuitive\nandthusW isonlyusedwhenmatchingdimensions.\nphenomenaaboutthedegradationproblem(Fig.1,left). As s\nThe form of the residual function is flexible. Exper-\nwe discussed in the introduction, if the added layers can\nF\niments in this paper involve a function that has two or\nbeconstructedasidentitymappings,adeepermodelshould\nF\nthreelayers(Fig.5),whilemorelayersarepossible. Butif\nhave training error no greater than its shallower counter-\nhasonlyasinglelayer,Eqn.(1)issimilartoalinearlayer:\npart. The degradation problem suggests that the solvers\nF\ny=W x+x,forwhichwehavenotobservedadvantages.\nmighthavedifficultiesinapproximatingidentitymappings 1\nWealsonotethatalthoughtheabovenotationsareabout\nbymultiplenonlinearlayers. Withtheresiduallearningre-\nfully-connectedlayersforsimplicity,theyareapplicableto\nformulation, if identity mappings are optimal, the solvers\nconvolutional layers. The function (x, W ) can repre-\nmaysimplydrivetheweightsofthemultiplenonlinearlay- i\nF { }\nsentmultipleconvolutionallayers. Theelement-wiseaddi-\nerstowardzerotoapproachidentitymappings.\ntionisperformedontwofeaturemaps,channelbychannel.\nInrealcases,itisunlikelythatidentitymappingsareop-\ntimal, but our reformulation may help to precondition the\n3.3.NetworkArchitectures\nproblem. If the optimal function is closer to an identity\nmappingthantoazeromapping,itshouldbeeasierforthe Wehavetestedvariousplain/residualnets,andhaveob-\nsolvertofindtheperturbationswithreferencetoanidentity servedconsistentphenomena. Toprovideinstancesfordis-\nmapping,thantolearnthefunctionasanewone. Weshow cussion,wedescribetwomodelsforImageNetasfollows.\nbyexperiments(Fig.7)thatthelearnedresidualfunctionsin\nPlain Network. Our plain baselines (Fig. 3, middle) are\ngeneralhavesmallresponses,suggestingthatidentitymap-\nmainlyinspiredbythephilosophyofVGGnets[41](Fig.3,\npingsprovidereasonablepreconditioning.\nleft). Theconvolutionallayersmostlyhave3 3filtersand\n\u00d7\n3.2.IdentityMappingbyShortcuts follow two simple design rules: (i) for the same output\nfeature map size, the layers have the same number of fil-\nWe adopt residual learning to every few stacked layers. ters; and (ii) if the feature map size is halved, the num-\nAbuildingblockisshowninFig.2. Formally,inthispaper ber of filters is doubled so as to preserve the time com-\nweconsiderabuildingblockdefinedas: plexity per layer. We perform downsampling directly by\nconvolutional layers that have a stride of 2. The network\ny= (x, W )+x. (1)\nF { i } ends with a global average pooling layer and a 1000-way\nfully-connected layer with softmax. The total number of\nHere x and y are the input and output vectors of the lay-\nweightedlayersis34inFig.3(middle).\ners considered. The function (x, W ) represents the\nF { i } It isworth noticing thatour model has fewer filters and\nresidual mapping to be learned. For the example in Fig. 2\nlowercomplexitythanVGGnets[41](Fig.3,left).Our34-\nthat has two layers, = W \u03c3(W x) in which \u03c3 denotes\n2 1\nF layerbaselinehas3.6billionFLOPs(multiply-adds),which\n2Thishypothesis,however,isstillanopenquestion.See[28]. isonly18%ofVGG-19(19.6billionFLOPs).\n3\nVGG-19 34-layer plain 34-layer residual ResidualNetwork. Basedontheaboveplainnetwork,we\ninsert shortcut connections (Fig. 3, right) which turn the\nimage image image\ns o iz u e t : p 2 u 2 t 4 3x3 conv, 64 network into its counterpart residual version. The identity\nshortcuts(Eqn.(1))canbedirectlyusedwhentheinputand\n3x3 conv, 64\noutput are of the same dimensions (solid line shortcuts in\npool, /2\noutput Fig.3).Whenthedimensionsincrease(dottedlineshortcuts\nsize: 112 3x3 conv, 128\nin Fig. 3), we consider two options: (A) The shortcut still\n3x3 conv, 128 7x7 conv, 64, /2 7x7 conv, 64, /2\nperforms identity mapping, with extra zero entries padded\npool, /2 pool, /2 pool, /2\noutput forincreasingdimensions. Thisoptionintroducesnoextra\nsize: 56 3x3 conv, 256 3x3 conv, 64 3x3 conv, 64 parameter;(B)TheprojectionshortcutinEqn.(2)isusedto\n3x3 conv, 256 3x3 conv, 64 3x3 conv, 64 match dimensions (done by 1 1 convolutions). For both\n\u00d7\n3x3 conv, 256 3x3 conv, 64 3x3 conv, 64 options, when the shortcuts go across feature maps of two\n3x3 conv, 256 3x3 conv, 64 3x3 conv, 64 sizes,theyareperformedwithastrideof2.\n3x3 conv, 64 3x3 conv, 64\n3.4.Implementation\n3x3 conv, 64 3x3 conv, 64\npool, /2 3x3 conv, 128, /2 3x3 conv, 128, /2 Our implementation for ImageNet follows the practice\noutput\nsize: 28 3x3 conv, 512 3x3 conv, 128 3x3 conv, 128 in [21, 41]. The image is resized with its shorter side ran-\n3x3 conv, 512 3x3 conv, 128 3x3 conv, 128 domly sampled in [256,480] for scale augmentation [41].\n3x3 conv, 512 3x3 conv, 128 3x3 conv, 128 A224 224cropisrandomlysampledfromanimageorits\n\u00d7\nhorizontalflip,withtheper-pixelmeansubtracted[21].The\n3x3 conv, 512 3x3 conv, 128 3x3 conv, 128\nstandardcoloraugmentationin[21]isused.Weadoptbatch\n3x3 conv, 128 3x3 conv, 128\nnormalization (BN) [16] right after each convolution and\n3x3 conv, 128 3x3 conv, 128\nbeforeactivation,following[16]. Weinitializetheweights\n3x3 conv, 128 3x3 conv, 128\nasin[13]andtrainallplain/residualnetsfromscratch. We\ns o iz u e t : p 1 u 4 t pool, /2 3x3 conv, 256, /2 3x3 conv, 256, /2 use SGD with a mini-batch size of 256. The learning rate\n3x3 conv, 512 3x3 conv, 256 3x3 conv, 256\nstartsfrom0.1andisdividedby10whentheerrorplateaus,\n3x3 conv, 512 3x3 conv, 256 3x3 conv, 256 andthemodelsaretrainedforupto60 104iterations. We\n3x3 conv, 512 3x3 conv, 256 3x3 conv, 256 \u00d7\nuseaweightdecayof0.0001andamomentumof0.9. We\n3x3 conv, 512 3x3 conv, 256 3x3 conv, 256 donotusedropout[14],followingthepracticein[16].\n3x3 conv, 256 3x3 conv, 256 Intesting,forcomparisonstudiesweadoptthestandard\n3x3 conv, 256 3x3 conv, 256 10-crop testing [21]. For best results, we adopt the fully-\n3x3 conv, 256 3x3 conv, 256 convolutional form as in [41, 13], and average the scores\n3x3 conv, 256 3x3 conv, 256 at multiple scales (images are resized such that the shorter\n3x3 conv, 256 3x3 conv, 256 sideisin 224,256,384,480,640 ).\n{ }\n3x3 conv, 256 3x3 conv, 256\n4.Experiments\n3x3 conv, 256 3x3 conv, 256\no si u z t e p : u 7 t pool, /2 3x3 conv, 512, /2 3x3 conv, 512, /2 4.1.ImageNetClassification\n3x3 conv, 512 3x3 conv, 512\nWeevaluateourmethodontheImageNet2012classifi-\n3x3 conv, 512 3x3 conv, 512\ncationdataset[36]thatconsistsof1000classes.Themodels\n3x3 conv, 512 3x3 conv, 512\nare trained on the 1.28 million training images, and evalu-\n3x3 conv, 512 3x3 conv, 512\nated on the 50k validation images. We also obtain a final\n3x3 conv, 512 3x3 conv, 512\nresult on the 100k test images, reported by the test server.\no si u z t e p : u 1 t fc 4096 avg pool avg pool Weevaluatebothtop-1andtop-5errorrates.\nfc 4096 fc 1000 fc 1000\nfc 1000 Plain Networks. We first evaluate 18-layer and 34-layer\nplainnets. The34-layerplainnetisinFig.3(middle). The\nFigure3.ExamplenetworkarchitecturesforImageNet. Left: the 18-layerplainnetisofasimilarform. SeeTable1forde-\nVGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid- tailedarchitectures.\ndle:aplainnetworkwith34parameterlayers(3.6billionFLOPs). TheresultsinTable2showthatthedeeper34-layerplain\nRight: a residual network with 34 parameter layers (3.6 billion net has higher validation error than the shallower 18-layer\nFLOPs).Thedottedshortcutsincreasedimensions.Table1shows plain net. To reveal the reasons, in Fig. 4 (left) we com-\nmoredetailsandothervariants.\nparetheirtraining/validationerrorsduringthetrainingpro-\ncedure. We have observed the degradation problem - the\n4\nlayername outputsize 18-layer 34-layer 50-layer 101-layer 152-layer\nconv1 112 112 7 7,64,stride2\n\u00d7 \u00d7\n3 3maxpool,stride2\nconv2x 56 \u00d7 56 (cid:20) 3 3 \u00d7 3 3 , , 6 6 4 4 (cid:21) \u00d7 2 (cid:20) 3 3 \u00d7 3 3 , , 6 6 4 4 (cid:21) \u00d7 3 \uf8ee \uf8f0 \u00d7 1 3 \u00d7 \u00d7 1 3 , , 6 6 4 4 \uf8f9 \uf8fb \u00d7 3 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 1 3 , , 6 6 4 4 \uf8f9 \uf8fb \u00d7 3 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 1 3 , , 6 6 4 4 \uf8f9 \uf8fb \u00d7 3\n\u00d7 \u00d7 1 1,256 1 1,256 1 1,256\nconv3x 28 \u00d7 28 (cid:20) 3 3 \u00d7 3 3 , , 1 1 2 2 8 8 (cid:21) \u00d7 2 (cid:20) 3 3 \u00d7 3 3 , , 1 1 2 2 8 8 (cid:21) \u00d7 4 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 \u00d7 1 3 , , 1 1 2 2 8 8 \uf8f9 \uf8fb \u00d7 4 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 \u00d7 1 3 , , 1 1 2 2 8 8 \uf8f9 \uf8fb \u00d7 4 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 \u00d7 1 3 , , 1 1 2 2 8 8 \uf8f9 \uf8fb \u00d7 8\n\u00d7 \u00d7 1 1,512 1 1,512 1 1,512\nconv4x 14 \u00d7 14 (cid:20) 3 3 \u00d7 3 3 , , 2 2 5 5 6 6 (cid:21) \u00d7 2 (cid:20) 3 3 \u00d7 3 3 , , 2 2 5 5 6 6 (cid:21) \u00d7 6 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 \u00d7 1 3 , , 2 2 5 5 6 6 \uf8f9 \uf8fb \u00d7 6 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 \u00d7 1 3 , , 2 2 5 5 6 6 \uf8f9 \uf8fb \u00d7 23 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 \u00d7 1 3 , , 2 2 5 5 6 6 \uf8f9 \uf8fb \u00d7 36\n\u00d7 \u00d7 1 1,1024 1 1,1024 1 1,1024\nconv5x 7 \u00d7 7 (cid:20) 3 3 \u00d7 3 3 , , 5 5 1 1 2 2 (cid:21) \u00d7 2 (cid:20) 3 3 \u00d7 3 3 , , 5 5 1 1 2 2 (cid:21) \u00d7 3 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 \u00d7 1 3 , , 5 5 1 1 2 2 \uf8f9 \uf8fb \u00d7 3 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 \u00d7 1 3 , , 5 5 1 1 2 2 \uf8f9 \uf8fb \u00d7 3 \uf8ee \uf8f0 1 3 \u00d7 \u00d7 \u00d7 1 3 , , 5 5 1 1 2 2 \uf8f9 \uf8fb \u00d7 3\n\u00d7 \u00d7 1 1,2048 1 1,2048 1 1,2048\n\u00d7 \u00d7 \u00d7\n1 1 averagepool,1000-dfc,softmax\n\u00d7\nFLOPs 1.8 109 3.6 109 3.8 109 7.6 109 11.3 109\n\u00d7 \u00d7 \u00d7 \u00d7 \u00d7\nTable1.ArchitecturesforImageNet. Buildingblocksareshowninbrackets(seealsoFig.5),withthenumbersofblocksstacked. Down-\nsamplingisperformedbyconv3 1,conv4 1,andconv5 1withastrideof2.\n60\n50\n40\n30\n20\n0 10 20 30 40 50\niter. (1e4)\n)%(\nrorre\n60\n50\n40\n30\nplain-18\nplain-34\n20\n0 10 20 30 40 50\niter. (1e4)\n)%(\nrorre\n34-layer\n18-layer\n18-layer\nResNet-18\nResNet-34 34-layer\nFigure4.TrainingonImageNet.Thincurvesdenotetrainingerror,andboldcurvesdenotevalidationerrorofthecentercrops.Left:plain\nnetworksof18and34layers.Right:ResNetsof18and34layers.Inthisplot,theresidualnetworkshavenoextraparametercomparedto\ntheirplaincounterparts.\nplain ResNet reducing of the training error3. The reason for such opti-\n18layers 27.94 27.88 mizationdifficultieswillbestudiedinthefuture.\n34layers 28.54 25.03\nResidual Networks. Next we evaluate 18-layer and 34-\nTable2.Top-1error(%,10-croptesting)onImageNetvalidation. layer residual nets (ResNets). The baseline architectures\nHeretheResNetshavenoextraparametercomparedtotheirplain arethesameastheaboveplainnets,expectthatashortcut\ncounterparts.Fig.4showsthetrainingprocedures.\nconnectionisaddedtoeachpairof3 3filtersasinFig.3\n\u00d7\n(right). In the first comparison (Table 2 and Fig. 4 right),\nweuseidentitymappingforallshortcutsandzero-padding\n34-layer plain net has higher training error throughout the forincreasingdimensions(optionA).Sotheyhavenoextra\nwhole training procedure, even though the solution space parametercomparedtotheplaincounterparts.\nof the 18-layer plain network is a subspace of that of the We have three major observations from Table 2 and\n34-layerone. Fig. 4. First, the situation is reversed with residual learn-\nWe argue that this optimization difficulty is unlikely to ing\u2013the34-layerResNetisbetterthanthe18-layerResNet\nbecausedbyvanishinggradients. Theseplainnetworksare (by2.8%). Moreimportantly,the34-layerResNetexhibits\ntrained with BN [16], which ensures forward propagated considerablylowertrainingerrorandisgeneralizabletothe\nsignalstohavenon-zerovariances. Wealsoverifythatthe validationdata. Thisindicatesthatthedegradationproblem\nbackwardpropagatedgradientsexhibithealthynormswith is well addressed in this setting and we manage to obtain\nBN. So neither forward nor backward signals vanish. In accuracygainsfromincreaseddepth.\nfact, the 34-layer plain net is still able to achieve compet- Second, compared to its plain counterpart, the 34-layer\nitive accuracy (Table 3), suggesting that the solver works\n3Wehaveexperimentedwithmoretrainingiterations(3\u00d7)andstillob-\ntosomeextent. Weconjecturethatthedeepplainnetsmay\nservedthedegradationproblem, suggestingthatthisproblemcannotbe\nhaveexponentiallylowconvergencerates,whichimpactthe feasiblyaddressedbysimplyusingmoreiterations.\n5\n64-d 256-d\nmodel top-1err. top-5err.\nVGG-16[41] 28.07 9.33 3x3, 64 1x1, 64\nrelu\nGoogLeNet[44] - 9.15 relu 3x3, 64\nPReLU-net[13] 24.27 7.38 3x3, 64 relu\n1x1, 256\nplain-34 28.54 10.02\nrelu relu\nResNet-34A 25.03 7.76\nResNet-34B 24.52 7.46\nFigure 5. A deeper residual function F for ImageNet. Left: a\nResNet-34C 24.19 7.40\nbuildingblock(on56\u00d756featuremaps)asinFig.3forResNet-\nResNet-50 22.85 6.71\n34.Right:a\u201cbottleneck\u201dbuildingblockforResNet-50/101/152.\nResNet-101 21.75 6.05\nResNet-152 21.43 5.71\nparameter-free, identity shortcuts help with training. Next\nTable3.Errorrates(%,10-croptesting)onImageNetvalidation.\nweinvestigateprojectionshortcuts(Eqn.(2)). InTable3we\nVGG-16isbasedonourtest. ResNet-50/101/152areofoptionB\ncomparethreeoptions: (A)zero-paddingshortcutsareused\nthatonlyusesprojectionsforincreasingdimensions.\nforincreasingdimensions,andallshortcutsareparameter-\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\nmethod top-1err. top-5err.\nVGG[41](ILSVRC\u201914) - 8.43\u2020 tionshortcutsareusedforincreasingdimensions,andother\nshortcutsareidentity;and(C)allshortcutsareprojections.\nGoogLeNet[44](ILSVRC\u201914) - 7.89\nTable3showsthatallthreeoptionsareconsiderablybet-\nVGG[41](v5) 24.4 7.1\nterthantheplaincounterpart.BisslightlybetterthanA.We\nPReLU-net[13] 21.59 5.71\narguethatthisisbecausethezero-paddeddimensionsinA\nBN-inception[16] 21.99 5.81\nindeedhavenoresiduallearning.Cismarginallybetterthan\nResNet-34B 21.84 5.71\nB, and we attribute this to the extra parameters introduced\nResNet-34C 21.53 5.60\nby many (thirteen) projection shortcuts. But the small dif-\nResNet-50 20.74 5.25\nferencesamongA/B/Cindicatethatprojectionshortcutsare\nResNet-101 19.87 4.60\nnotessentialforaddressingthedegradationproblem.Sowe\nResNet-152 19.38 4.49\ndonotuseoptionCintherestofthispaper,toreducemem-\nTable4.Errorrates(%)ofsingle-modelresultsontheImageNet ory/timecomplexityandmodelsizes. Identityshortcutsare\nvalidationset(except\u2020reportedonthetestset).\nparticularly important for not increasing the complexity of\nthebottleneckarchitecturesthatareintroducedbelow.\nmethod top-5err.(test)\nVGG[41](ILSVRC\u201914) 7.32 Deeper Bottleneck Architectures. Next we describe our\nGoogLeNet[44](ILSVRC\u201914) 6.66 deepernetsforImageNet.Becauseofconcernsonthetrain-\nVGG[41](v5) 6.8 ing time that we can afford, we modify the building block\nPReLU-net[13] 4.94 as a bottleneck design4. For each residual function , we\nF\nBN-inception[16] 4.82 useastackof3layersinsteadof2(Fig.5). Thethreelayers\nResNet(ILSVRC\u201915) 3.57 are1 1,3 3,and1 1convolutions,wherethe1 1layers\n\u00d7 \u00d7 \u00d7 \u00d7\nareresponsibleforreducingandthenincreasing(restoring)\nTable5.Errorrates(%)ofensembles. Thetop-5errorisonthe\ndimensions,leavingthe3 3layerabottleneckwithsmaller\ntestsetofImageNetandreportedbythetestserver. \u00d7\ninput/output dimensions. Fig. 5 shows an example, where\nbothdesignshavesimilartimecomplexity.\nResNetreducesthetop-1errorby3.5%(Table2),resulting Theparameter-freeidentityshortcutsareparticularlyim-\nfromthesuccessfullyreducedtrainingerror(Fig.4rightvs. portantforthebottleneckarchitectures.Iftheidentityshort-\nleft). Thiscomparisonverifiestheeffectivenessofresidual cut in Fig. 5 (right) is replaced with projection, one can\nlearningonextremelydeepsystems. showthatthetimecomplexityandmodelsizearedoubled,\nas the shortcut is connected to the two high-dimensional\nLast, we also note that the 18-layer plain/residual nets\nends. So identity shortcuts lead to more efficient models\narecomparablyaccurate(Table2),butthe18-layerResNet\nforthebottleneckdesigns.\nconvergesfaster(Fig.4rightvs.left). Whenthenetis\u201cnot\n50-layer ResNet: We replace each 2-layer block in the\noverlydeep\u201d(18layershere),thecurrentSGDsolverisstill\nabletofindgoodsolutionstotheplainnet. Inthiscase,the\n4Deepernon-bottleneckResNets(e.g.,Fig.5left)alsogainaccuracy\nResNet eases the optimization by providing faster conver-\nfromincreaseddepth(asshownonCIFAR-10),butarenotaseconomical\ngenceattheearlystage. asthebottleneckResNets.Sotheusageofbottleneckdesignsismainlydue\ntopracticalconsiderations. Wefurthernotethatthedegradationproblem\nIdentity vs. Projection Shortcuts. We have shown that ofplainnetsisalsowitnessedforthebottleneckdesigns.\n6\n34-layernetwiththis3-layerbottleneckblock,resultingin method error(%)\na50-layerResNet(Table1).WeuseoptionBforincreasing Maxout[10] 9.38\ndimensions. Thismodelhas3.8billionFLOPs. NIN[25] 8.81\n101-layer and 152-layer ResNets: We construct 101- DSN[24] 8.22\nlayer and 152-layer ResNets by using more 3-layer blocks #layers #params\n(Table 1). Remarkably, although the depth is significantly FitNet[35] 19 2.5M 8.39\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still Highway[42,43] 19 2.3M 7.54(7.72\u00b10.16)\nhas lower complexity than VGG-16/19 nets (15.3/19.6 bil- Highway[42,43] 32 1.25M 8.80\nlionFLOPs). ResNet 20 0.27M 8.75\nThe 50/101/152-layer ResNets are more accurate than ResNet 32 0.46M 7.51\nthe34-layeronesbyconsiderablemargins(Table3and4). ResNet 44 0.66M 7.17\nWe do not observe the degradation problem and thus en- ResNet 56 0.85M 6.97\njoysignificantaccuracygainsfromconsiderablyincreased ResNet 110 1.7M 6.43(6.61\u00b10.16)\ndepth.Thebenefitsofdeptharewitnessedforallevaluation ResNet 1202 19.4M 7.93\nmetrics(Table3and4).\nTable6.ClassificationerrorontheCIFAR-10testset. Allmeth-\nComparisons with State-of-the-art Methods. In Table 4\nodsarewithdataaugmentation.ForResNet-110,werunit5times\nwe compare with the previous best single-model results. andshow\u201cbest(mean\u00b1std)\u201dasin[43].\nOurbaseline34-layerResNetshaveachievedverycompet-\nitive accuracy. Our 152-layer ResNet has a single-model\nsoourresidualmodelshaveexactlythesamedepth,width,\ntop-5 validation error of 4.49%. This single-model result\nandnumberofparametersastheplaincounterparts.\noutperforms all previous ensemble results (Table 5). We\nWeuseaweightdecayof0.0001andmomentumof0.9,\ncombinesixmodelsofdifferentdepthtoformanensemble\nandadopttheweightinitializationin[13]andBN[16]but\n(only with two 152-layer ones at the time of submitting).\nwith no dropout. These models are trained with a mini-\nThis leads to 3.57% top-5 error on the test set (Table 5).\nbatch size of 128 on two GPUs. We start with a learning\nThisentrywonthe1stplaceinILSVRC2015.\nrate of 0.1, divide it by 10 at 32k and 48k iterations, and\n4.2.CIFAR-10andAnalysis terminatetrainingat64kiterations,whichisdeterminedon\na45k/5ktrain/valsplit. Wefollowthesimpledataaugmen-\nWe conducted more studies on the CIFAR-10 dataset\ntationin[24]fortraining: 4pixelsarepaddedoneachside,\n[20], which consists of 50k training images and 10k test-\nand a 32 32 crop is randomly sampled from the padded\ning images in 10 classes. We present experiments trained \u00d7\nimage or its horizontal flip. For testing, we only evaluate\nonthetrainingsetandevaluatedonthetestset. Ourfocus\nthesingleviewoftheoriginal32 32image.\nisonthebehaviorsofextremelydeepnetworks,butnoton \u00d7\nWecomparen = 3,5,7,9 ,leadingto20,32,44,and\npushingthestate-of-the-artresults, soweintentionallyuse { }\n56-layernetworks. Fig.6(left)showsthebehaviorsofthe\nsimplearchitecturesasfollows.\nplainnets. Thedeepplainnetssufferfromincreaseddepth,\nTheplain/residualarchitecturesfollowtheforminFig.3\nand exhibit higher training error when going deeper. This\n(middle/right). Thenetworkinputsare32 32images,with\nphenomenonissimilartothatonImageNet(Fig.4,left)and\n\u00d7\ntheper-pixelmeansubtracted. Thefirstlayeris3 3convo-\nonMNIST(see[42]),suggestingthatsuchanoptimization\n\u00d7\nlutions. Thenweuseastackof6nlayerswith3 3convo-\ndifficultyisafundamentalproblem.\n\u00d7\nlutionsonthefeaturemapsofsizes 32,16,8 respectively,\nFig. 6 (middle) shows the behaviors of ResNets. Also\n{ }\nwith 2n layers for each feature map size. The numbers of\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\nfiltersare 16,32,64 respectively.Thesubsamplingisper-\nmanagetoovercometheoptimizationdifficultyanddemon-\n{ }\nformedbyconvolutionswithastrideof2.Thenetworkends\nstrateaccuracygainswhenthedepthincreases.\nwith a global average pooling, a 10-way fully-connected\nWe further explore n = 18 that leads to a 110-layer\nlayer,andsoftmax.Therearetotally6n+2stackedweighted\nResNet. In this case, we find that the initial learning rate\nlayers. Thefollowingtablesummarizesthearchitecture: of 0.1 is slightly too large to start converging5. So we use\n0.01towarmupthetraininguntilthetrainingerrorisbelow\noutputmapsize 32\u00d732 16\u00d716 8\u00d78\n80%(about400iterations),andthengobackto0.1andcon-\n#layers 1+2n 2n 2n\ntinuetraining. Therestofthelearningscheduleisasdone\n#filters 16 32 64\npreviously. This110-layernetworkconvergeswell(Fig.6,\nmiddle). It has fewer parameters than other deep and thin\nWhen shortcut connections are used, they are connected\nto the pairs of 3 3 layers (totally 3n shortcuts). On this 5Withaninitiallearningrateof0.1,itstartsconverging(<90%error)\n\u00d7\ndatasetweuseidentityshortcutsinallcases(i.e.,optionA), afterseveralepochs,butstillreachessimilaraccuracy.\n7\n20\n10\n5\n00 1 2 3 4 5 6\niter. (1e4)\n)%(\nrorre\n20\n10\nplain-20 5\nplain-32\nplain-44\nplain-56\n00 1 2 3 4 5 6\niter. (1e4)\n)%(\nrorre\n20\nResNet-20\nResNet-32\nResNet-44\nResNet-56\n56-layer ResNet-110\n20-layer 20-layer 10\n110-layer\n5\n1\n0 4 5 6\niter. (1e4)\n)%(\nrorre\nresidual-110\nresidual-1202\nFigure6.TrainingonCIFAR-10. Dashedlinesdenotetrainingerror,andboldlinesdenotetestingerror. Left: plainnetworks. Theerror\nofplain-110ishigherthan60%andnotdisplayed.Middle:ResNets.Right:ResNetswith110and1202layers.\n3\n2\n1\n0 20 40 60 80 100\nlayer index (sorted by magnitude)\ndts\n3\n2\n1\n0 20 40 60 80 100\nlayer index (original)\nplain-20 plain-56\nResNet-20\nResNet-56\nResNet-110\ndts\nplain-20 trainingdata 07+12 07++12 plain-56\nResNet-20 testdata VOC07test VOC12test\nResNet-56\nResNet-110 VGG-16 73.2 70.4\nResNet-101 76.4 73.8\nTable 7. Object detection mAP (%) on the PASCAL VOC\n2007/2012 test sets using baseline Faster R-CNN. See also Ta-\nble10and11forbetterresults.\nmetric mAP@.5 mAP@[.5,.95]\nVGG-16 41.5 21.2\nResNet-101 48.4 27.2\nFigure7.Standarddeviations(std)oflayerresponsesonCIFAR-\n10.Theresponsesaretheoutputsofeach3\u00d73layer,afterBNand Table 8. Object detection mAP (%) on the COCO validation set\nbefore nonlinearity. Top: the layers are shown in their original usingbaselineFasterR-CNN.SeealsoTable9forbetterresults.\norder.Bottom:theresponsesarerankedindescendingorder.\nhavesimilartrainingerror. Wearguethatthisisbecauseof\nnetworks such as FitNet [35] and Highway [42] (Table 6), overfitting. The 1202-layer network may be unnecessarily\nyetisamongthestate-of-the-artresults(6.43%,Table6). large (19.4M) for this small dataset. Strong regularization\nsuchasmaxout[10]ordropout[14]isappliedtoobtainthe\nAnalysis of Layer Responses. Fig. 7 shows the standard\nbestresults([10,25,24,35])onthisdataset. Inthispaper,\ndeviations (std) of the layer responses. The responses are\nweusenomaxout/dropoutandjustsimplyimposeregular-\nthe outputs of each 3 3 layer, after BN and before other\nization via deep and thin architectures by design, without\n\u00d7\nnonlinearity (ReLU/addition). For ResNets, this analy-\ndistracting from the focus on the difficulties of optimiza-\nsis reveals the response strength of the residual functions.\ntion. But combining with stronger regularization may im-\nFig.7showsthatResNetshavegenerallysmallerresponses\nproveresults,whichwewillstudyinthefuture.\nthantheirplaincounterparts. Theseresultssupportourba-\nsic motivation (Sec.3.1) that the residual functions might 4.3.ObjectDetectiononPASCALandMSCOCO\nbegenerallyclosertozerothanthenon-residualfunctions.\nOur method has good generalization performance on\nWe also notice that the deeper ResNet has smaller magni-\notherrecognitiontasks. Table7and 8showtheobjectde-\ntudesofresponses,asevidencedbythecomparisonsamong\ntection baseline results on PASCAL VOC 2007 and 2012\nResNet-20, 56, and 110 in Fig. 7. When there are more\n[5]andCOCO[26].WeadoptFasterR-CNN[32]asthede-\nlayers, an individual layer of ResNets tends to modify the\ntectionmethod.Hereweareinterestedintheimprovements\nsignalless.\nofreplacingVGG-16[41]withResNet-101. Thedetection\nExploring Over 1000 layers. We explore an aggressively implementation(seeappendix)ofusingbothmodelsisthe\ndeep model of over 1000 layers. We set n = 200 that same,sothegainscanonlybeattributedtobetternetworks.\nleadstoa1202-layernetwork,whichistrainedasdescribed Mostremarkably,onthechallengingCOCOdatasetweob-\nabove. Our method shows no optimization difficulty, and taina6.0%increaseinCOCO\u2019sstandardmetric(mAP@[.5,\nthis 103-layer network is able to achieve training error .95]), which is a 28% relative improvement. This gain is\n<0.1% (Fig. 6, right). Its test error is still fairly good solelyduetothelearnedrepresentations.\n(7.93%,Table6). Based on deep residual nets, we won the 1st places in\nBut there are still open problems on such aggressively severaltracksinILSVRC&COCO2015competitions:Im-\ndeepmodels. Thetestingresultofthis1202-layernetwork ageNetdetection,ImageNetlocalization,COCOdetection,\nisworsethanthatofour110-layernetwork, althoughboth andCOCOsegmentation. Thedetailsareintheappendix.\n8\nReferences\n[28] G.Montu\u00b4far,R.Pascanu,K.Cho,andY.Bengio. Onthenumberof\nlinearregionsofdeepneuralnetworks.InNIPS,2014.\n[1] Y.Bengio,P.Simard,andP.Frasconi.Learninglong-termdependen-\n[29] V.NairandG.E.Hinton. Rectifiedlinearunitsimproverestricted\ncieswithgradientdescentisdifficult. IEEETransactionsonNeural\nboltzmannmachines.InICML,2010.\nNetworks,5(2):157\u2013166,1994.\n[30] F.PerronninandC.Dance.Fisherkernelsonvisualvocabulariesfor\n[2] C. M. Bishop. Neural networks for pattern recognition. Oxford\nimagecategorization.InCVPR,2007.\nuniversitypress,1995.\n[31] T.Raiko,H.Valpola,andY.LeCun. Deeplearningmadeeasierby\n[3] W.L.Briggs,S.F.McCormick,etal. AMultigridTutorial. Siam,\nlineartransformationsinperceptrons.InAISTATS,2012.\n2000.\n[32] S.Ren, K.He, R.Girshick, andJ.Sun. FasterR-CNN:Towards\n[4] K.Chatfield,V.Lempitsky,A.Vedaldi,andA.Zisserman.Thedevil\nreal-timeobjectdetectionwithregionproposalnetworks. InNIPS,\nisinthedetails: anevaluationofrecentfeatureencodingmethods.\n2015.\nInBMVC,2011.\n[33] S.Ren,K.He,R.Girshick,X.Zhang,andJ.Sun. Objectdetection\n[5] M.Everingham,L.VanGool,C.K.Williams,J.Winn,andA.Zis-\nnetworksonconvolutionalfeaturemaps.arXiv:1504.06066,2015.\nserman. ThePascalVisualObjectClasses(VOC)Challenge. IJCV,\n[34] B.D.Ripley. Patternrecognitionandneuralnetworks. Cambridge\npages303\u2013338,2010.\nuniversitypress,1996.\n[6] S.GidarisandN.Komodakis.Objectdetectionviaamulti-region&\n[35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nsemanticsegmentation-awarecnnmodel.InICCV,2015.\nY.Bengio.Fitnets:Hintsforthindeepnets.InICLR,2015.\n[7] R.Girshick.FastR-CNN.InICCV,2015.\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\n[8] R.Girshick,J.Donahue,T.Darrell,andJ.Malik. Richfeaturehier-\nZ.Huang, A.Karpathy, A.Khosla, M.Bernstein, etal. Imagenet\narchiesforaccurateobjectdetectionandsemanticsegmentation. In\nlargescalevisualrecognitionchallenge.arXiv:1409.0575,2014.\nCVPR,2014.\n[37] A.M.Saxe,J.L.McClelland,andS.Ganguli. Exactsolutionsto\n[9] X.GlorotandY.Bengio. Understandingthedifficultyoftraining\nthenonlineardynamicsoflearningindeeplinearneuralnetworks.\ndeepfeedforwardneuralnetworks.InAISTATS,2010.\narXiv:1312.6120,2013.\n[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\n[38] N.N.Schraudolph.Acceleratedgradientdescentbyfactor-centering\nY.Bengio.Maxoutnetworks.arXiv:1302.4389,2013.\ndecomposition.Technicalreport,1998.\n[11] K.HeandJ.Sun.Convolutionalneuralnetworksatconstrainedtime\n[39] N.N.Schraudolph. Centeringneuralnetworkgradientfactors. In\ncost.InCVPR,2015.\nNeural Networks: Tricks of the Trade, pages 207\u2013226. Springer,\n[12] K.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpoolingindeep 1998.\nconvolutionalnetworksforvisualrecognition.InECCV,2014.\n[40] P.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,andY.Le-\n[13] K.He,X.Zhang,S.Ren,andJ.Sun. Delvingdeepintorectifiers: Cun. Overfeat: Integrated recognition, localization and detection\nSurpassinghuman-levelperformanceonimagenetclassification. In usingconvolutionalnetworks.InICLR,2014.\nICCV,2015.\n[41] K.SimonyanandA.Zisserman. Verydeepconvolutionalnetworks\n[14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and forlarge-scaleimagerecognition.InICLR,2015.\nR.R.Salakhutdinov. Improvingneuralnetworksbypreventingco-\n[42] R.K.Srivastava,K.Greff,andJ.Schmidhuber. Highwaynetworks.\nadaptationoffeaturedetectors.arXiv:1207.0580,2012.\narXiv:1505.00387,2015.\n[15] S.HochreiterandJ.Schmidhuber.Longshort-termmemory.Neural\n[43] R.K.Srivastava,K.Greff,andJ.Schmidhuber. Trainingverydeep\ncomputation,9(8):1735\u20131780,1997.\nnetworks.1507.06228,2015.\n[16] S.IoffeandC.Szegedy. Batchnormalization: Acceleratingdeep\n[44] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Er-\nnetworktrainingbyreducinginternalcovariateshift.InICML,2015.\nhan,V.Vanhoucke,andA.Rabinovich. Goingdeeperwithconvolu-\n[17] H.Jegou,M.Douze,andC.Schmid.Productquantizationfornearest tions.InCVPR,2015.\nneighborsearch.TPAMI,33,2011.\n[45] R.Szeliski. Fastsurfaceinterpolationusinghierarchicalbasisfunc-\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and tions.TPAMI,1990.\nC.Schmid.Aggregatinglocalimagedescriptorsintocompactcodes.\n[46] R.Szeliski. Locallyadaptedhierarchicalbasispreconditioning. In\nTPAMI,2012.\nSIGGRAPH,2006.\n[19] Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Girshick,\n[47] T.Vatanen,T.Raiko,H.Valpola,andY.LeCun. Pushingstochas-\nS.Guadarrama,andT.Darrell.Caffe:Convolutionalarchitecturefor\nticgradienttowardssecond-ordermethods\u2013backpropagationlearn-\nfastfeatureembedding.arXiv:1408.5093,2014.\ning with transformations in nonlinearities. In Neural Information\n[20] A.Krizhevsky. Learningmultiplelayersoffeaturesfromtinyim- Processing,2013.\nages.TechReport,2009.\n[48] A.VedaldiandB.Fulkerson. VLFeat:Anopenandportablelibrary\n[21] A.Krizhevsky,I.Sutskever,andG.Hinton. Imagenetclassification ofcomputervisionalgorithms,2008.\nwithdeepconvolutionalneuralnetworks.InNIPS,2012.\n[49] W.VenablesandB.Ripley. Modernappliedstatisticswiths-plus.\n[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, 1999.\nW.Hubbard, andL.D.Jackel. Backpropagationappliedtohand-\n[50] M.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolu-\nwrittenzipcoderecognition.Neuralcomputation,1989.\ntionalneuralnetworks.InECCV,2014.\n[23] Y.LeCun,L.Bottou,G.B.Orr,andK.-R.Mu\u00a8ller.Efficientbackprop.\nInNeuralNetworks:TricksoftheTrade,pages9\u201350.Springer,1998.\n[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-\nsupervisednets.arXiv:1409.5185,2014.\n[25] M.Lin,Q.Chen,andS.Yan.Networkinnetwork.arXiv:1312.4400,\n2013.\n[26] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,\nP.Dolla\u00b4r,andC.L.Zitnick. MicrosoftCOCO:Commonobjectsin\ncontext.InECCV.2014.\n[27] J.Long,E.Shelhamer,andT.Darrell. Fullyconvolutionalnetworks\nforsemanticsegmentation.InCVPR,2015.\n9\nA.ObjectDetectionBaselines 8 images (i.e., 1 per GPU) and the Fast R-CNN step has a\nmini-batch size of 16 images. The RPN step and Fast R-\nInthissectionweintroduceourdetectionmethodbased\nCNNsteparebothtrainedfor240kiterationswithalearn-\nonthebaselineFasterR-CNN[32]system. Themodelsare\ningrateof0.001andthenfor80kiterationswith0.0001.\ninitializedbytheImageNetclassificationmodels,andthen\nTable 8 shows the results on the MS COCO validation\nfine-tuned on the object detection data. We have experi-\nset. ResNet-101hasa6%increaseofmAP@[.5,.95]over\nmented with ResNet-50/101 at the time of the ILSVRC &\nVGG-16,whichisa28%relativeimprovement,solelycon-\nCOCO2015detectioncompetitions.\ntributedbythefeatureslearnedbythebetternetwork. Re-\nUnlikeVGG-16usedin[32],ourResNethasnohidden\nmarkably,themAP@[.5,.95]\u2019sabsoluteincrease(6.0%)is\nfc layers. We adopt the idea of \u201cNetworks on Conv fea-\nnearly as big as mAP@.5\u2019s (6.9%). This suggests that a\nture maps\u201d (NoC) [33] to address this issue. We compute\ndeepernetworkcanimprovebothrecognitionandlocaliza-\nthe full-image shared conv feature maps using those lay-\ntion.\nerswhosestridesontheimagearenogreaterthan16pixels\n(i.e.,conv1,conv2 x,conv3 x,andconv4 x,totally91conv\nB.ObjectDetectionImprovements\nlayersinResNet-101;Table1). Weconsidertheselayersas\nanalogous to the 13 conv layers in VGG-16, and by doing\nForcompleteness,wereporttheimprovementsmadefor\nso,bothResNetandVGG-16haveconvfeaturemapsofthe\nthe competitions. These improvements are based on deep\nsame total stride (16 pixels). These layers are shared by a\nfeaturesandthusshouldbenefitfromresiduallearning.\nregion proposal network (RPN, generating 300 proposals)\n[32] and a Fast R-CNN detection network [7]. RoI pool- MSCOCO\ning [7] is performed before conv5 1. On this RoI-pooled Boxrefinement. Ourboxrefinementpartiallyfollowstheit-\nfeature, all layers of conv5 x and up are adopted for each erativelocalizationin[6].InFasterR-CNN,thefinaloutput\nregion, playing the roles of VGG-16\u2019s fc layers. The final isaregressedboxthatisdifferentfromitsproposalbox. So\nclassificationlayerisreplacedbytwosiblinglayers(classi- forinference,wepoolanewfeaturefromtheregressedbox\nficationandboxregression[7]). and obtain a new classification score and a new regressed\nFor the usage of BN layers, after pre-training, we com- box. Wecombinethese300newpredictionswiththeorig-\nputetheBNstatistics(meansandvariances)foreachlayer inal300predictions. Non-maximumsuppression(NMS)is\nontheImageNettrainingset. ThentheBNlayersarefixed applied on the union set of predicted boxes using an IoU\nduring fine-tuning for object detection. As such, the BN threshold of 0.3 [8], followed by box voting [6]. Box re-\nlayers become linear activations with constant offsets and finementimprovesmAPbyabout2points(Table9).\nscales,andBNstatisticsarenotupdatedbyfine-tuning. We\nGlobal context. We combine global context in the Fast\nfixtheBNlayersmainlyforreducingmemoryconsumption\nR-CNN step. Given the full-image conv feature map, we\ninFasterR-CNNtraining.\npoolafeaturebyglobalSpatialPyramidPooling[12](with\nPASCALVOC a \u201csingle-level\u201d pyramid) which can be implemented as\nFollowing [7, 32], for the PASCAL VOC 2007 test set, \u201cRoI\u201dpoolingusingtheentireimage\u2019sboundingboxasthe\nweusethe5ktrainvalimagesinVOC2007and16ktrain- RoI. This pooled feature is fed into the post-RoI layers to\nval images in VOC 2012 for training (\u201c07+12\u201d). For the obtainaglobalcontextfeature. Thisglobalfeatureiscon-\nPASCAL VOC 2012 test set, we use the 10k trainval+test catenated with the original per-region feature, followed by\nimagesinVOC2007and16ktrainvalimagesinVOC2012 the sibling classification and box regression layers. This\nfor training (\u201c07++12\u201d). The hyper-parameters for train- new structure is trained end-to-end. Global context im-\ning Faster R-CNN are the same as in [32]. Table 7 shows provesmAP@.5byabout1point(Table9).\nthe results. ResNet-101 improves the mAP by >3% over\nMulti-scaletesting. Intheabove,allresultsareobtainedby\nVGG-16. Thisgainissolelybecauseoftheimprovedfea-\nsingle-scale training/testing as in [32], where the image\u2019s\ntureslearnedbyResNet.\nshortersideiss = 600pixels. Multi-scaletraining/testing\nMSCOCO has been developed in [12, 7] by selecting a scale from a\nThe MS COCO dataset [26] involves 80 object cate- feature pyramid, and in [33] by using maxout layers. In\ngories. We evaluate the PASCAL VOC metric (mAP @ ourcurrentimplementation,wehaveperformedmulti-scale\nIoU=0.5)andthestandardCOCOmetric(mAP@IoU= testing following [33]; we have not performed multi-scale\n.5:.05:.95). Weusethe80kimagesonthetrainsetfortrain- trainingbecauseoflimitedtime. Inaddition,wehaveper-\ning and the 40k images on the val set for evaluation. Our formed multi-scale testing only for the Fast R-CNN step\ndetectionsystemforCOCOissimilartothatforPASCAL (but not yet for the RPN step). With a trained model, we\nVOC. We train the COCO models with an 8-GPU imple- computeconvfeaturemapsonanimagepyramid,wherethe\nmentation, and thus the RPN step has a mini-batch size of image\u2019s shorter sides are s 200,400,600,800,1000 .\n\u2208 { }\n10\ntrainingdata COCOtrain COCOtrainval\ntestdata COCOval COCOtest-dev\nmAP @.5 @[.5,.95] @.5 @[.5,.95]\nbaselineFasterR-CNN(VGG-16) 41.5 21.2\nbaselineFasterR-CNN(ResNet-101) 48.4 27.2\n+boxrefinement 49.9 29.9\n+context 51.1 30.0 53.3 32.2\n+multi-scaletesting 53.8 32.5 55.7 34.9\nensemble 59.0 37.4\nTable9.ObjectdetectionimprovementsonMSCOCOusingFasterR-CNNandResNet-101.\nsystem net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nbaseline VGG-16 07+12 73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\nbaseline ResNet-101 07+12 76.4 79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0\nbaseline+++ ResNet-101 COCO+07+12 85.6 90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8\nTable10.DetectionresultsonthePASCALVOC2007testset. ThebaselineistheFasterR-CNNsystem. Thesystem\u201cbaseline+++\u201d\nincludeboxrefinement,context,andmulti-scaletestinginTable9.\nsystem net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\nbaseline VGG-16 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nbaseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6\nbaseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0\nTable 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/\ndisplaylb.php?challengeid=11&compid=4). ThebaselineistheFasterR-CNNsystem. Thesystem\u201cbaseline+++\u201dinclude\nboxrefinement,context,andmulti-scaletestinginTable9.\nWe select two adjacent scales from the pyramid following val2 test\n[33]. RoI pooling and subsequent layers are performed on GoogLeNet[44](ILSVRC\u201914) - 43.9\nthefeaturemapsofthesetwoscales[33],whicharemerged\noursinglemodel(ILSVRC\u201915) 60.5 58.8\nbymaxoutasin[33].Multi-scaletestingimprovesthemAP\nourensemble(ILSVRC\u201915) 63.6 62.1\nbyover2points(Table9).\nTable12.Ourresults(mAP,%)ontheImageNetdetectiondataset.\nUsingvalidationdata.Nextweusethe80k+40ktrainvalset OurdetectionsystemisFasterR-CNN[32]withtheimprovements\nfortrainingandthe20ktest-devsetforevaluation.Thetest- inTable9,usingResNet-101.\ndevsethasnopubliclyavailablegroundtruthandtheresult\nisreportedbytheevaluationserver. Underthissetting,the\nweachieve85.6%mAPonPASCALVOC2007(Table10)\nresultsareanmAP@.5of55.7%andanmAP@[.5,.95]of and83.8%onPASCALVOC2012(Table11)6. Theresult\n34.9%(Table9). Thisisoursingle-modelresult.\nonPASCALVOC2012is10pointshigherthantheprevi-\nEnsemble.InFasterR-CNN,thesystemisdesignedtolearn ousstate-of-the-artresult[6].\nregionproposalsandalsoobjectclassifiers,soanensemble\nImageNetDetection\ncan be used to boost both tasks. We use an ensemble for\nTheImageNetDetection(DET)taskinvolves200object\nproposing regions, and the union set of proposals are pro-\ncategories. The accuracy is evaluated by mAP@.5. Our\ncessed by an ensemble of per-region classifiers. Table 9\nobject detection algorithm for ImageNet DET is the same\nshowsourresultbasedonanensembleof3networks. The\nas that for MS COCO in Table 9. The networks are pre-\nmAP is 59.0% and 37.4% on the test-dev set. This result\ntrained on the 1000-class ImageNet classification set, and\nwonthe1stplaceinthedetectiontaskinCOCO2015.\narefine-tunedontheDETdata. Wesplitthevalidationset\nPASCALVOC into two parts (val1/val2) following [8]. We fine-tune the\nWerevisitthePASCALVOCdatasetbasedontheabove detection models using the DET training set and the val1\nmodel. WiththesinglemodelontheCOCOdataset(55.7% set. Theval2setisusedforvalidation. Wedonotuseother\nmAP@.5inTable9), wefine-tunethismodelonthePAS- ILSVRC2015data. OursinglemodelwithResNet-101has\nCALVOCsets. Theimprovementsofboxrefinement,con-\n6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html,\ntext, andmulti-scaletestingarealsoadopted. Bydoingso submittedon2015-11-26.\n11\nLOC LOC LOCerror classification top-5LOCerror top-5localizationerr\ntesting method\nmethod network onGTCLS network onpredictedCLS val test\nVGG\u2019s[41] VGG-16 1-crop 33.1[41]\nOverFeat[40](ILSVRC\u201913) 30.0 29.9\nRPN ResNet-101 1-crop 13.3\nRPN ResNet-101 dense 11.7 GoogLeNet[44](ILSVRC\u201914) - 26.7\nRPN ResNet-101 dense ResNet-101 14.4 VGG[41](ILSVRC\u201914) 26.9 25.3\nRPN+RCNN ResNet-101 dense ResNet-101 10.6 ours(ILSVRC\u201915) 8.9 9.0\nRPN+RCNN ensemble dense ensemble 8.9\nTable14.Comparisonsoflocalizationerror(%)ontheImageNet\nTable13. Localizationerror(%) ontheImageNetvalidation. In\ndatasetwithstate-of-the-artmethods.\nthecolumnof\u201cLOCerroronGTclass\u201d([41]), thegroundtruth\nclass is used. In the \u201ctesting\u201d column, \u201c1-crop\u201d denotes testing\nonacentercropof224\u00d7224pixels,\u201cdense\u201ddenotesdense(fully portsacenter-croperrorof33.1%(Table13)usingground\nconvolutional)andmulti-scaletesting. truthclasses. Underthesamesetting,ourRPNmethodus-\ningResNet-101netsignificantlyreducesthecenter-croper-\nror to 13.3%. This comparison demonstrates the excellent\n58.8%mAPandourensembleof3modelshas62.1%mAP\nperformanceofourframework. Withdense(fullyconvolu-\nontheDETtestset(Table12).Thisresultwonthe1stplace\ntional)andmulti-scaletesting,ourResNet-101hasanerror\nintheImageNetdetectiontaskinILSVRC2015,surpassing\nof11.7%usinggroundtruthclasses. UsingResNet-101for\nthesecondplaceby8.5points(absolute).\npredictingclasses(4.6%top-5classificationerror,Table4),\nthetop-5localizationerroris14.4%.\nC.ImageNetLocalization\nTheaboveresultsareonlybasedontheproposalnetwork\n(RPN) in Faster R-CNN [32]. One may use the detection\nTheImageNetLocalization(LOC)task[36]requiresto\nnetwork(FastR-CNN[7])inFasterR-CNNtoimprovethe\nclassify and localize the objects. Following [40, 41], we\nresults.Butwenoticethatonthisdataset,oneimageusually\nassumethattheimage-levelclassifiersarefirstadoptedfor\ncontainsasingledominateobject,andtheproposalregions\npredicting the class labels of an image, and the localiza-\nhighly overlap with each other and thus have very similar\ntionalgorithmonlyaccountsforpredictingboundingboxes\nRoI-pooledfeatures. Asaresult,theimage-centrictraining\nbasedonthepredictedclasses. Weadoptthe\u201cper-classre-\nof Fast R-CNN [7] generates samples of small variations,\ngression\u201d(PCR)strategy[40,41],learningaboundingbox\nwhichmaynotbedesiredforstochastictraining. Motivated\nregressorforeachclass. Wepre-trainthenetworksforIm-\nby this, in our current experiment we use the original R-\nageNet classification and then fine-tune them for localiza-\nCNN[8]thatisRoI-centric,inplaceofFastR-CNN.\ntion. We train networks on the provided 1000-class Ima-\nOurR-CNNimplementationisasfollows. Weapplythe\ngeNettrainingset.\nper-class RPN trained as above on the training images to\nOur localization algorithm is based on the RPN frame-\npredict bounding boxes for the ground truth class. These\nwork of [32] with a few modifications. Unlike the way in\npredicted boxes play a role of class-dependent proposals.\n[32] that is category-agnostic, our RPN for localization is\nFor each training image, the highest scored 200 proposals\ndesignedinaper-classform. ThisRPNendswithtwosib-\nareextractedastrainingsamplestotrainanR-CNNclassi-\nling1 1convolutionallayersforbinaryclassification(cls)\n\u00d7 fier. Theimageregioniscroppedfromaproposal,warped\nandboxregression(reg),asin[32]. Theclsandreglayers\nto 224 224 pixels, and fed into the classification network\nare both in a per-class from, in contrast to [32]. Specifi- \u00d7\nasinR-CNN[8].Theoutputsofthisnetworkconsistoftwo\ncally,theclslayerhasa1000-doutput,andeachdimension\nsibling fc layers for cls and reg, also in a per-class form.\nisbinarylogisticregressionforpredictingbeingornotbe-\nThis R-CNN network is fine-tuned on the training set us-\ning an object class; the reg layer has a 1000 4-d output\n\u00d7 ingamini-batchsizeof256intheRoI-centricfashion. For\nconsisting of box regressors for 1000 classes. As in [32],\ntesting,theRPNgeneratesthehighestscored200proposals\nour bounding box regression is with reference to multiple\nforeachpredictedclass,andtheR-CNNnetworkisusedto\ntranslation-invariant\u201canchor\u201dboxesateachposition.\nupdatetheseproposals\u2019scoresandboxpositions.\nAsinourImageNetclassificationtraining(Sec.3.4),we\nThis method reduces the top-5 localization error to\nrandomly sample 224 224 crops for data augmentation.\n\u00d7 10.6% (Table 13). This is our single-model result on the\nWeuseamini-batchsizeof256imagesforfine-tuning. To\nvalidationset.Usinganensembleofnetworksforbothclas-\navoidnegativesamplesbeingdominate, 8anchorsareran-\nsification and localization, we achieve a top-5 localization\ndomlysampledforeachimage,wherethesampledpositive\nerrorof9.0%onthetestset. Thisnumbersignificantlyout-\nand negative anchors have a ratio of 1:1 [32]. For testing,\nperformstheILSVRC14results(Table14),showinga64%\nthenetworkisappliedontheimagefully-convolutionally.\nrelativereductionoferror. Thisresultwonthe1stplacein\nTable 13 compares the localization results. Following\ntheImageNetlocalizationtaskinILSVRC2015.\n[41],wefirstperform\u201coracle\u201dtestingusingthegroundtruth\nclassastheclassificationprediction. VGG\u2019spaper[41]re-\n12",
  "sections": {
    "abstract": "Abstract 20\nDeeper neural networks are more difficult to train. We\npresentaresiduallearningframeworktoeasethetraining 10\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ningresidualfunctionswithreferencetothelayerinputs,in- 0 0 1 2 iter. 3 (1e4) 4 5 6\nsteadoflearningunreferencedfunctions. Weprovidecom-\nprehensive empirical evidence showing that these residual\nnetworksareeasiertooptimize,andcangainaccuracyfrom\nconsiderablyincreaseddepth. OntheImageNetdatasetwe\nevaluateresidualnetswithadepthofupto152layers\u20148\n\u00d7 deeperthanVGGnets[41]butstillhavinglowercomplex-\nity.Anensembleoftheseresidualnetsachieves3.57%error\nontheImageNettestset.Thisresultwonthe1stplaceonthe\nILSVRC2015classificationtask. Wealsopresentanalysis\nonCIFAR-10with100and1000layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremelydeeprepresentations,weobtaina28%relativeim-\nprovement on the COCO object detection dataset. Deep\nresidualnetsarefoundationsofoursubmissionstoILSVRC\n& COCO 2015 competitions1, where we also won the 1st\nplacesonthetasksofImageNetdetection,ImageNetlocal-\nization,COCOdetection,andCOCOsegmentation.\n1.Introduction\nDeep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classification [21,\n50, 40]. Deep networks naturally integrate low/mid/high-\nlevel features [50] and classifiers in an end-to-end multi-\nlayer fashion, and the \u201clevels\u201d of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[41,44]revealsthatnetworkdepthisofcrucialimportance,\nand the leading results [41, 44, 13, 16] on the challenging\nImageNetdataset[36]allexploit\u201cverydeep\u201d[41]models,\nwithadepthofsixteen[41]tothirty[16]. Manyothernon-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/ and\nhttp://mscoco.org/dataset/#detections-challenge2015.\n)%(\nrorre\ngniniart\n20\n10\n00 1",
    "introduction": "Introduction\nDeep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classification [21,\n50, 40]. Deep networks naturally integrate low/mid/high-\nlevel features [50] and classifiers in an end-to-end multi-\nlayer fashion, and the \u201clevels\u201d of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[41,44]revealsthatnetworkdepthisofcrucialimportance,\nand the leading results [41, 44, 13, 16] on the challenging\nImageNetdataset[36]allexploit\u201cverydeep\u201d[41]models,\nwithadepthofsixteen[41]tothirty[16]. Manyothernon-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/ and\nhttp://mscoco.org/dataset/#detections-challenge2015.\n)%(\nrorre\ngniniart\n20\n10\n00 1 2 3 4 5 6 iter. (1e4)\n)%(\nrorre\ntset\n56-layer\n20-layer\n56-layer\n20-layer\nFigure1.Trainingerror(left)andtesterror(right)onCIFAR-10\nwith20-layerand56-layer\u201cplain\u201dnetworks. Thedeepernetwork\nhashighertrainingerror,andthustesterror. Similarphenomena\nonImageNetispresentedinFig.4.\ngreatlybenefitedfromverydeepmodels.\nDrivenbythesignificanceofdepth,aquestionarises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [1, 9], which\nhamper convergence from the beginning. This problem,\nhowever,hasbeenlargelyaddressedbynormalizedinitial-\nization[23,9,37,13]andintermediatenormalizationlayers\n[16],whichenablenetworkswithtensoflayerstostartcon-\nverging for stochastic gradient descent (SGD) with back-\npropagation[22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly. Unexpectedly,\nsuch degradation is not caused by overfitting, and adding\nmorelayerstoasuitablydeepmodelleadstohighertrain-\ningerror,asreportedin[11,42]andthoroughlyverifiedby\nourexperiments. Fig.1showsatypical",
    "method": "method [3] reformulates the system as subprob-\nofnonlinearlayers. lems at multiple scales, where each subproblem is respon-\nsiblefortheresidualsolutionbetweenacoarserandafiner\nTheformulationof (x)+xcanberealizedbyfeedfor-\nF scale. AnalternativetoMultigridishierarchicalbasispre-\nwardneuralnetworkswith\u201cshortcutconnections\u201d(Fig.2).\nconditioning[45,46], whichreliesonvariablesthatrepre-\nShortcut connections [2, 34, 49] are those skipping one or\nsentresidualvectorsbetweentwoscales. Ithasbeenshown\nmore layers. In our case, the shortcut connections simply\n[3,45,46]thatthesesolversconvergemuchfasterthanstan-\nperform identity mapping, and their outputs are added to\ndard solvers that are unaware of the residual nature of the\nthe outputs of the stacked layers (Fig. 2). Identity short-\nsolutions.Thesemethodssuggestthatagoodreformulation\ncut connections add neither extra parameter nor computa-\norpreconditioningcansimplifytheoptimization.\ntional complexity. The entire network can still be trained\nend-to-endbySGDwithbackpropagation,andcanbeeas-\nShortcut Connections. Practices and theories that lead to\nily implemented using common libraries (e.g., Caffe [19])\nshortcutconnections[2,34,49]havebeenstudiedforalong\nwithoutmodifyingthesolvers.\ntime. Anearlypracticeoftrainingmulti-layerperceptrons\nWe present comprehensive experiments on ImageNet (MLPs)istoaddalinearlayerconnectedfromthenetwork\n[36] to show the degradation problem and evaluate our input to the output [34, 49]. In [44, 24], a few interme-\nmethod. Weshowthat: 1)Ourextremelydeepresidualnets diate layers are directly connected to auxiliary classifiers\nare easy to optimize, but the counterpart \u201cplain\u201d nets (that for addressing vanishing/exploding gradients. The papers\nsimply stack layers) exhibit higher training error when the of[39,38,31,47]proposemethodsforcenteringlayerre-\ndepthincreases;2)Ourdeepresidualnetscaneasilyenjoy sponses,gradients,andpropagatederrors,implementedby\naccuracygainsfromgreatlyincreaseddepth,producingre- ",
    "model": "models,\nwithadepthofsixteen[41]tothirty[16]. Manyothernon-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/ and\nhttp://mscoco.org/dataset/#detections-challenge2015.\n)%(\nrorre\ngniniart\n20\n10\n00 1 2 3 4 5 6 iter. (1e4)\n)%(\nrorre\ntset\n56-layer\n20-layer\n56-layer\n20-layer\nFigure1.Trainingerror(left)andtesterror(right)onCIFAR-10\nwith20-layerand56-layer\u201cplain\u201dnetworks. Thedeepernetwork\nhashighertrainingerror,andthustesterror. Similarphenomena\nonImageNetispresentedinFig.4.\ngreatlybenefitedfromverydeepmodels.\nDrivenbythesignificanceofdepth,aquestionarises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [1, 9], which\nhamper convergence from the beginning. This problem,\nhowever,hasbeenlargelyaddressedbynormalizedinitial-\nization[23,9,37,13]andintermediatenormalizationlayers\n[16],whichenablenetworkswithtensoflayerstostartcon-\nverging for stochastic gradient descent (SGD) with back-\npropagation[22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly. Unexpectedly,\nsuch degradation is not caused by overfitting, and adding\nmorelayerstoasuitablydeepmodelleadstohighertrain-\ningerror,asreportedin[11,42]andthoroughlyverifiedby\nourexperiments. Fig.1showsatypicalexample.\nThedegradation(oftrainingaccuracy)indicatesthatnot\nallsystemsaresimilarlyeasytooptimize. Letusconsidera\nshallower architecture and its deeper counterpart that adds\nmorelayersontoit. Thereexistsasolutionbyconstruction\ntothedeepermodel: theaddedlayersareidentitymapping,\nand the other layers are copied from the learned shallower\nmodel. Theexistenceofthisconstructedsolutionindicates\nthatadeepermodelshouldproducenohighertrainingerror\nthan its shallower counterpart. But experiments show that\nourcurrentsolve",
    "algorithm": "algorithms,2008.\nwithdeepconvolutionalneuralnetworks.InNIPS,2012.\n[49] W.VenablesandB.Ripley. Modernappliedstatisticswiths-plus.\n[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, 1999.\nW.Hubbard, andL.D.Jackel. Backpropagationappliedtohand-\n[50] M.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolu-\nwrittenzipcoderecognition.Neuralcomputation,1989.\ntionalneuralnetworks.InECCV,2014.\n[23] Y.LeCun,L.Bottou,G.B.Orr,andK.-R.Mu\u00a8ller.Efficientbackprop.\nInNeuralNetworks:TricksoftheTrade,pages9\u201350.Springer,1998.\n[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-\nsupervisednets.arXiv:1409.5185,2014.\n[25] M.Lin,Q.Chen,andS.Yan.Networkinnetwork.arXiv:1312.4400,\n2013.\n[26] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,\nP.Dolla\u00b4r,andC.L.Zitnick. MicrosoftCOCO:Commonobjectsin\ncontext.InECCV.2014.\n[27] J.Long,E.Shelhamer,andT.Darrell. Fullyconvolutionalnetworks\nforsemanticsegmentation.InCVPR,2015.\n9\nA.ObjectDetectionBaselines 8 images (i.e., 1 per GPU) and the Fast R-CNN step has a\nmini-batch size of 16 images. The RPN step and Fast R-\nInthissectionweintroduceourdetectionmethodbased\nCNNsteparebothtrainedfor240kiterationswithalearn-\nonthebaselineFasterR-CNN[32]system. Themodelsare\ningrateof0.001andthenfor80kiterationswith0.0001.\ninitializedbytheImageNetclassificationmodels,andthen\nTable 8 shows the results on the MS COCO validation\nfine-tuned on the object detection data. We have experi-\nset. ResNet-101hasa6%increaseofmAP@[.5,.95]over\nmented with ResNet-50/101 at the time of the ILSVRC &\nVGG-16,whichisa28%relativeimprovement,solelycon-\nCOCO2015detectioncompetitions.\ntributedbythefeatureslearnedbythebetternetwork. Re-\nUnlikeVGG-16usedin[32],ourResNethasnohidden\nmarkably,themAP@[.5,.95]\u2019sabsoluteincrease(6.0%)is\nfc layers. We adopt the idea of \u201cNetworks on Conv fea-\nnearly as big as mAP@.5\u2019s (6.9%). This suggests that a\nture maps\u201d (NoC) [33] to address this issue. We compute\ndeepernetworkcanimprovebothrecognitionandlocaliza-\nthe full-image s",
    "experiments": "experiments. Fig.1showsatypicalexample.\nThedegradation(oftrainingaccuracy)indicatesthatnot\nallsystemsaresimilarlyeasytooptimize. Letusconsidera\nshallower architecture and its deeper counterpart that adds\nmorelayersontoit. Thereexistsasolutionbyconstruction\ntothedeepermodel: theaddedlayersareidentitymapping,\nand the other layers are copied from the learned shallower\nmodel. Theexistenceofthisconstructedsolutionindicates\nthatadeepermodelshouldproducenohighertrainingerror\nthan its shallower counterpart. But experiments show that\nourcurrentsolversonhandareunabletofindsolutionsthat\n1\n5102\nceD\n01\n]VC.sc[\n1v58330.2151:viXra\nImageNet test set, and won the 1st place in the ILSVRC\nx\n2015 classification competition. The extremely deep rep-\nweight layer\nresentationsalsohaveexcellentgeneralizationperformance\nF (x) relu x on other recognition tasks, and lead us to further win the\nweight layer\nidentity 1st places on: ImageNet detection, ImageNet localization,\nCOCO detection, and COCO segmentation in ILSVRC &\n(x)(cid:1)+(cid:1)x\nF relu COCO2015competitions. Thisstrongevidenceshowsthat\nFigure2.Residuallearning:abuildingblock. theresiduallearningprincipleisgeneric,andweexpectthat\nitisapplicableinothervisionandnon-visionproblems.\narecomparablygoodorbetterthantheconstructedsolution\n(orunabletodosoinfeasibletime). 2.RelatedWork\nIn this paper, we address the degradation problem by\nintroducing a deep residual learning framework. In- Residual Representations. In image recognition, VLAD\nstead of hoping each few stacked layers directly fit a [18]isarepresentationthatencodesbytheresidualvectors\ndesired underlying mapping, we explicitly let these lay- with respect to a dictionary, and Fisher Vector [30] can be\ners fit a residual mapping. Formally, denoting the desired formulated as a probabilistic version [18] of VLAD. Both\nunderlyingmappingas (x), weletthestackednonlinear ofthemarepowerfulshallowrepresentationsforimagere-\nlayersfitanothermappin H gof (x):= (x) x. Theorig- trieval and classific",
    "results": "results [41, 44, 13, 16] on the challenging\nImageNetdataset[36]allexploit\u201cverydeep\u201d[41]models,\nwithadepthofsixteen[41]tothirty[16]. Manyothernon-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/ and\nhttp://mscoco.org/dataset/#detections-challenge2015.\n)%(\nrorre\ngniniart\n20\n10\n00 1 2 3 4 5 6 iter. (1e4)\n)%(\nrorre\ntset\n56-layer\n20-layer\n56-layer\n20-layer\nFigure1.Trainingerror(left)andtesterror(right)onCIFAR-10\nwith20-layerand56-layer\u201cplain\u201dnetworks. Thedeepernetwork\nhashighertrainingerror,andthustesterror. Similarphenomena\nonImageNetispresentedinFig.4.\ngreatlybenefitedfromverydeepmodels.\nDrivenbythesignificanceofdepth,aquestionarises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [1, 9], which\nhamper convergence from the beginning. This problem,\nhowever,hasbeenlargelyaddressedbynormalizedinitial-\nization[23,9,37,13]andintermediatenormalizationlayers\n[16],whichenablenetworkswithtensoflayerstostartcon-\nverging for stochastic gradient descent (SGD) with back-\npropagation[22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly. Unexpectedly,\nsuch degradation is not caused by overfitting, and adding\nmorelayerstoasuitablydeepmodelleadstohighertrain-\ningerror,asreportedin[11,42]andthoroughlyverifiedby\nourexperiments. Fig.1showsatypicalexample.\nThedegradation(oftrainingaccuracy)indicatesthatnot\nallsystemsaresimilarlyeasytooptimize. Letusconsidera\nshallower architecture and its deeper counterpart that adds\nmorelayersontoit. Thereexistsasolutionbyconstruction\ntothedeepermodel: theaddedlayersareidentitymapping,\nand the other layers are copied from the learned shallower\nmodel. Theexistenceofthisconstructedsolutionindicates\nthatadeepermodelshouldproducenohigher"
  }
}