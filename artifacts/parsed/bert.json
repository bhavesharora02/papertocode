{
  "raw_text": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\nJacobDevlin Ming-WeiChang KentonLee KristinaToutanova\nGoogleAILanguage\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\nAbstract There are two existing strategies for apply-\ningpre-trainedlanguage representations todown-\nWe introduce a new language representa-\nstream tasks: feature-based and fine-tuning. The\ntion model called BERT, which stands for\nfeature-based approach, such as ELMo (Peters\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre- et al., 2018a), uses task-specific architectures that\nsentation models (Peters et al., 2018a; Rad- include the pre-trained representations as addi-\nford et al., 2018), BERT is designed to pre- tional features. The fine-tuning approach, such as\ntrain deep bidirectional representations from the Generative Pre-trained Transformer (OpenAI\nunlabeledtextbyjointlyconditioningonboth\nGPT) (Radford et al., 2018), introduces minimal\nleft and right context in all layers. As a re-\ntask-specific parameters, and is trained on the\nsult,thepre-trainedBERTmodelcanbefine-\ndownstream tasks by simply fine-tuning all pre-\ntuned with just one additional output layer\ntrainedparameters. Thetwoapproachessharethe\nto create state-of-the-art models for a wide\nrangeoftasks,suchasquestionansweringand sameobjectivefunctionduringpre-training,where\nlanguage inference, without substantial task- they use unidirectional language models to learn\nspecificarchitecturemodifications. generallanguagerepresentations.\nBERT is conceptually simple and empirically We argue that current techniques restrict the\npowerful. It obtains new state-of-the-art re- power of the pre-trained representations, espe-\nsults on eleven natural language processing cially for the fine-tuning approaches. The ma-\ntasks, including pushing the GLUE score to\njorlimitationisthatstandardlanguagemodelsare\n80.5% (7.7% point absolute improvement),\nunidirectional, and this limits the choice of archi-\nMultiNLI accuracy to 86.7% (4.6% absolute\ntectures that can be used during pre-training. For\nimprovement),SQuADv1.1questionanswer-\ning Test F1 to 93.2 (1.5 point absolute im- example,inOpenAIGPT,theauthorsusealeft-to-\nprovement) and SQuAD v2.0 Test F1 to 83.1 right architecture, where every token can only at-\n(5.1pointabsoluteimprovement). tendtoprevioustokensintheself-attentionlayers\noftheTransformer(Vaswanietal.,2017). Suchre-\n1 Introduction\nstrictionsaresub-optimalforsentence-leveltasks,\nLanguage model pre-training has been shown to and could be very harmful when applying fine-\nbe effective for improving many natural language tuningbasedapproachestotoken-leveltaskssuch\nprocessing tasks (Dai and Le, 2015; Peters et al., asquestionanswering,whereitiscrucialtoincor-\n2018a; Radford et al., 2018; Howard and Ruder, poratecontextfrombothdirections.\n2018). Theseincludesentence-leveltaskssuchas In this paper, we improve the fine-tuning based\nnatural language inference (Bowman et al., 2015; approaches by proposing BERT: Bidirectional\nWilliams et al., 2018) and paraphrasing (Dolan Encoder Representations from Transformers.\nand Brockett, 2005), which aim to predict the re- BERT alleviates the previously mentioned unidi-\nlationships between sentences by analyzing them rectionality constraint by using a \u201cmasked lan-\nholistically, as well as token-level tasks such as guage model\u201d (MLM) pre-training objective, in-\nnamedentityrecognitionandquestionanswering, spired by the Cloze task (Taylor, 1953). The\nwheremodelsarerequiredtoproducefine-grained maskedlanguagemodelrandomlymaskssomeof\noutput at the token level (Tjong Kim Sang and the tokens from the input, and the objective is to\nDeMeulder,2003;Rajpurkaretal.,2016). predict the original vocabulary id of the masked\n9102\nyaM\n42\n]LC.sc[\n2v50840.0181:viXra\nword based only on its context. Unlike left-to- These approaches have been generalized to\nright language model pre-training, the MLM ob- coarser granularities, such as sentence embed-\njective enables the representation to fuse the left dings (Kiros et al., 2015; Logeswaran and Lee,\nand the right context, which allows us to pre- 2018)orparagraphembeddings(LeandMikolov,\ntrain a deep bidirectional Transformer. In addi- 2014). To train sentence representations, prior\ntion to the masked language model, we also use work has used objectives to rank candidate next\na \u201cnext sentence prediction\u201d task that jointly pre- sentences (Jernite et al., 2017; Logeswaran and\ntrains text-pair representations. The contributions Lee, 2018), left-to-right generation of next sen-\nofourpaperareasfollows: tencewordsgivenarepresentationoftheprevious\nsentence (Kiros et al., 2015), or denoising auto-\n\u2022 Wedemonstratetheimportanceofbidirectional\nencoderderivedobjectives(Hilletal.,2016).\npre-training for language representations. Un-\nELMo and its predecessor (Peters et al., 2017,\nlikeRadfordetal.(2018),whichusesunidirec-\n2018a) generalize traditional word embedding re-\ntional language models for pre-training, BERT\nsearch along a different dimension. They extract\nuses masked language models to enable pre-\ncontext-sensitivefeaturesfromaleft-to-rightanda\ntraineddeep bidirectionalrepresentations. This\nright-to-left language model. The contextual rep-\nisalsoincontrasttoPetersetal.(2018a),which\nresentation of each token is the concatenation of\nuses a shallow concatenation of independently\nthe left-to-right and right-to-left representations.\ntrainedleft-to-rightandright-to-leftLMs.\nWhen integrating contextual word embeddings\n\u2022 Weshowthatpre-trainedrepresentationsreduce with existing task-specific architectures, ELMo\nthe need for many heavily-engineered task- advancesthestateoftheartforseveralmajorNLP\nspecific architectures. BERT is the first fine- benchmarks (Peters et al., 2018a) including ques-\ntuningbasedrepresentationmodelthatachieves tionanswering(Rajpurkaretal.,2016),sentiment\nstate-of-the-art performance on a large suite analysis (Socher et al., 2013), and named entity\nof sentence-level and token-level tasks, outper- recognition (Tjong Kim Sang and De Meulder,\nformingmanytask-specificarchitectures. 2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\n\u2022 BERT advances the state of the art for eleven\ndictasinglewordfrombothleftandrightcontext\nNLP tasks. The code and pre-trained mod-\nusing LSTMs. Similar to ELMo, their model is\nelsareavailableathttps://github.com/\nfeature-based and not deeply bidirectional. Fedus\ngoogle-research/bert.\netal.(2018)showsthattheclozetaskcanbeused\ntoimprovetherobustnessoftextgenerationmod-\n2 RelatedWork\nels.\nThereisalonghistoryofpre-traininggenerallan-\nguage representations, and we briefly review the\n2.2 UnsupervisedFine-tuningApproaches\nmostwidely-usedapproachesinthissection.\nAs with the feature-based approaches, the first\n2.1 UnsupervisedFeature-basedApproaches works in this direction only pre-trained word em-\nLearning widely applicable representations of bedding parameters from unlabeled text (Col-\nwords has been an active area of research for lobertandWeston,2008).\ndecades,includingnon-neural(Brownetal.,1992; More recently, sentence or document encoders\nAndo and Zhang, 2005; Blitzer et al., 2006) and which produce contextual token representations\nneural (Mikolov et al., 2013; Pennington et al., have been pre-trained from unlabeled text and\n2014) methods. Pre-trained word embeddings fine-tuned for a supervised downstream task (Dai\nare an integral part of modern NLP systems, of- and Le, 2015; Howard and Ruder, 2018; Radford\nfering significant improvements over embeddings et al., 2018). The advantage of these approaches\nlearnedfromscratch(Turianetal.,2010). Topre- is that few parameters need to be learned from\ntrain word embedding vectors, left-to-right lan- scratch. At least partly due to this advantage,\nguage modeling objectives have been used (Mnih OpenAIGPT(Radfordetal.,2018)achievedpre-\nand Hinton, 2009), as well as objectives to dis- viously state-of-the-art results on many sentence-\ncriminate correct from incorrect words in left and level tasks from the GLUE benchmark (Wang\nrightcontext(Mikolovetal.,2013). et al., 2018a). Left-to-right language model-\nNSP Mask LM Mask LM MNLI NER SQuAD Start/End Span\nC T ... T T T\u2019 ... T \u2019 C T ... T T T\u2019 ... T \u2019\n1 N [SEP] 1 M 1 N [SEP] 1 M\nBERT BERT BERT\nE[CLS] E\n1\n... E\nN\nE\n[SEP]\nE\n1\n\u2019 ... E\nM\n\u2019 E[CLS] E\n1\n... E\nN\nE\n[SEP]\nE\n1\n\u2019 ... E\nM\n\u2019\n[CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM\nMasked Sentence A Masked Sentence B Question Paragraph\nUnlabeled Sentence A and B Pair Question Answer Pair\nPre-training Fine-Tuning\nFigure1: Overallpre-trainingandfine-tuningproceduresforBERT.Apartfromoutputlayers,thesamearchitec-\nturesareusedinbothpre-trainingandfine-tuning. Thesamepre-trainedmodelparametersareusedtoinitialize\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\ning and auto-encoder objectives have been used mal difference between the pre-trained architec-\nfor pre-training such models (Howard and Ruder, tureandthefinaldownstreamarchitecture.\n2018;Radfordetal.,2018;DaiandLe,2015).\nModel Architecture BERT\u2019s model architec-\n2.3 TransferLearningfromSupervisedData tureisamulti-layerbidirectionalTransformeren-\ncoder based on the original implementation de-\nTherehasalsobeenworkshowingeffectivetrans-\nscribed in Vaswani et al. (2017) and released in\nferfromsupervisedtaskswithlargedatasets,such\nthe tensor2tensor library.1 Because the use\nas natural language inference (Conneau et al.,\nofTransformershasbecomecommonandourim-\n2017) and machine translation (McCann et al.,\nplementation is almost identical to the original,\n2017). Computervisionresearchhasalsodemon-\nwe will omit an exhaustive background descrip-\nstrated the importance of transfer learning from\ntionofthemodelarchitectureandreferreadersto\nlargepre-trainedmodels,whereaneffectiverecipe\nVaswani et al. (2017) as well as excellent guides\nis to fine-tune models pre-trained with Ima-\nsuchas\u201cTheAnnotatedTransformer.\u201d2\ngeNet(Dengetal.,2009;Yosinskietal.,2014).\nIn this work, we denote the number of layers\n3 BERT (i.e., Transformerblocks)asL, thehiddensizeas\nH, and the number of self-attention heads as A.3\nWe introduce BERT and its detailed implementa-\nWe primarily report results on two model sizes:\ntion in this section. There are two steps in our BERT (L=12, H=768, A=12, Total Param-\nBASE\nframework: pre-training and fine-tuning. Dur- eters=110M) and BERT (L=24, H=1024,\nLARGE\ningpre-training,themodelistrainedonunlabeled\nA=16,TotalParameters=340M).\ndata over different pre-training tasks. For fine-\nBERT waschosentohavethesamemodel\nBASE\ntuning, the BERT model is first initialized with\nsize as OpenAI GPT for comparison purposes.\nthe pre-trained parameters, and all of the param-\nCritically, however, the BERT Transformer uses\neters are fine-tuned using labeled data from the\nbidirectional self-attention, while the GPT Trans-\ndownstreamtasks. Eachdownstreamtaskhassep-\nformerusesconstrainedself-attentionwhereevery\narate fine-tuned models, eventhough theyare ini- tokencanonlyattendtocontexttoitsleft.4\ntializedwiththesamepre-trainedparameters. The\nquestion-answeringexampleinFigure1willserve 1https://github.com/tensorflow/tensor2tensor\nasarunningexampleforthissection.\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3Inallcaseswesetthefeed-forward/filtersizetobe4H,\nA distinctive feature of BERT is its unified ar-\ni.e.,3072fortheH =768and4096fortheH =1024.\nchitecture across different tasks. There is mini- 4We note that in the literature the bidirectional Trans-\nInput/OutputRepresentations TomakeBERT Inordertotrainadeepbidirectionalrepresenta-\nhandle a variety of down-stream tasks, our input tion,wesimplymasksomepercentageoftheinput\nrepresentation is able to unambiguously represent tokens at random, and then predict those masked\nboth a single sentence and a pair of sentences tokens. We refer to this procedure as a \u201cmasked\n(e.g.,(cid:104)Question,Answer(cid:105))inonetokensequence. LM\u201d (MLM), although it is often referred to as a\nThroughoutthiswork,a\u201csentence\u201dcanbeanarbi- Cloze task in the literature (Taylor, 1953). In this\ntraryspanofcontiguoustext,ratherthananactual case,thefinalhiddenvectorscorrespondingtothe\nlinguisticsentence. A\u201csequence\u201dreferstothein- mask tokens are fed into an output softmax over\nputtokensequencetoBERT,whichmaybeasin- the vocabulary, as in a standard LM. In all of our\nglesentenceortwosentencespackedtogether. experiments, we mask 15% of all WordPiece to-\nWe use WordPiece embeddings (Wu et al., kens in each sequence at random. In contrast to\n2016) with a 30,000 token vocabulary. The first denoisingauto-encoders(Vincentetal.,2008),we\ntoken of every sequence is always a special clas- only predict the masked words rather than recon-\nsification token ([CLS]). The final hidden state structingtheentireinput.\ncorresponding to this token is used as the ag- Although this allows us to obtain a bidirec-\ngregate sequence representation for classification tional pre-trained model, a downside is that we\ntasks. Sentence pairs are packed together into a are creating a mismatch between pre-training and\nsinglesequence. Wedifferentiatethesentencesin fine-tuning,sincethe[MASK]tokendoesnotap-\ntwo ways. First, we separate them with a special pear during fine-tuning. To mitigate this, we do\ntoken([SEP]).Second,weaddalearnedembed- not always replace \u201cmasked\u201d words with the ac-\nding to every token indicating whether it belongs tual [MASK] token. The training data generator\ntosentenceAorsentenceB.AsshowninFigure1, chooses15%ofthetokenpositionsatrandomfor\nwedenoteinputembeddingasE,thefinalhidden prediction. If the i-th token is chosen, we replace\nvector of the special [CLS] token as C \u2208 RH, thei-th token with (1) the[MASK]token 80% of\nand the final hidden vector for the ith input token the time (2) a random token 10% of the time (3)\nasT \u2208 RH. the unchanged i-th token 10% of the time. Then,\ni\nFor a given token, its input representation is T i will be used to predict the original token with\nconstructedbysummingthecorrespondingtoken, cross entropy loss. We compare variations of this\nsegment, and position embeddings. A visualiza- procedureinAppendixC.2.\ntionofthisconstructioncanbeseeninFigure2.\nTask #2: Next Sentence Prediction (NSP)\n3.1 Pre-trainingBERT Many important downstream tasks such as Ques-\ntionAnswering(QA)andNaturalLanguageInfer-\nUnlike Peters et al. (2018a) and Radford et al.\nence (NLI) are based on understanding the rela-\n(2018), we do not use traditional left-to-right or\ntionship between two sentences, which is not di-\nright-to-left language models to pre-train BERT.\nrectly captured by language modeling. In order\nInstead, we pre-train BERT using two unsuper-\nto train a model that understands sentence rela-\nvised tasks, described in this section. This step\ntionships, we pre-train for a binarized next sen-\nispresentedintheleftpartofFigure1.\ntence prediction task that can be trivially gener-\nTask #1: Masked LM Intuitively, it is reason- ated from any monolingual corpus. Specifically,\nable to believe that a deep bidirectional model is whenchoosingthesentencesAandBforeachpre-\nstrictly more powerful than either a left-to-right training example, 50% of the time B is the actual\nmodel or the shallow concatenation of a left-to- next sentence that follows A (labeled as IsNext),\nright and a right-to-left model. Unfortunately, and 50% of the time it is a random sentence from\nstandardconditionallanguagemodelscanonlybe the corpus (labeled as NotNext). As we show\ntrained left-to-right or right-to-left, since bidirec- in Figure 1, C is used for next sentence predic-\ntional conditioning would allow each word to in- tion (NSP).5 Despite its simplicity, we demon-\ndirectly \u201csee itself\u201d, and the model could trivially strate in Section 5.1 that pre-training towards this\npredictthetargetwordinamulti-layeredcontext. task is very beneficial to both QA and NLI. 6\nformerisoftenreferredtoasa\u201cTransformerencoder\u201dwhile 5Thefinalmodelachieves97%-98%accuracyonNSP.\ntheleft-context-onlyversionisreferredtoasa\u201cTransformer 6ThevectorCisnotameaningfulsentencerepresentation\ndecoder\u201dsinceitcanbeusedfortextgeneration. withoutfine-tuning,sinceitwastrainedwithNSP.\nInput [CLS] my dog is cute [SEP] he likes play ##ing [SEP]\nToken\nE E E E E E E E E E E\nEmbeddings [CLS] my dog is cute [SEP] he likes play ##ing [SEP]\nSegment\nE E E E E E E E E E E\nEmbeddings A A A A A A B B B B B\nPosition\nE E E E E E E E E E E\nEmbeddings 0 1 2 3 4 5 6 7 8 9 10\nFigure2: BERTinputrepresentation. Theinputembeddingsarethesumofthetokenembeddings,thesegmenta-\ntionembeddingsandthepositionembeddings.\nThe NSP task is closely related to representation- (4) a degenerate text-\u2205 pair in text classification\nlearningobjectivesusedinJerniteetal.(2017)and orsequencetagging. Attheoutput,thetokenrep-\nLogeswaran and Lee (2018). However, in prior resentationsarefedintoanoutputlayerfortoken-\nwork,onlysentenceembeddingsaretransferredto level tasks, such as sequence tagging or question\ndown-stream tasks, where BERT transfers all pa- answering, and the [CLS] representation is fed\nrameterstoinitializeend-taskmodelparameters. into an output layer for classification, such as en-\ntailmentorsentimentanalysis.\nPre-training data The pre-training procedure\nCompared to pre-training, fine-tuning is rela-\nlargely follows the existing literature on language\ntively inexpensive. All of the results in the pa-\nmodelpre-training. Forthepre-trainingcorpuswe\nper can be replicated in at most 1 hour on a sin-\nuse the BooksCorpus (800M words) (Zhu et al.,\ngleCloudTPU,orafewhoursonaGPU,starting\n2015) and English Wikipedia (2,500M words).\nfrom the exact same pre-trained model.7 We de-\nFor Wikipedia we extract only the text passages\nscribe the task-specific details in the correspond-\nand ignore lists, tables, and headers. It is criti-\ning subsections of Section 4. More details can be\ncal to use a document-level corpus rather than a\nfoundinAppendixA.5.\nshuffled sentence-level corpus such as the Billion\nWordBenchmark(Chelbaetal.,2013)inorderto 4 Experiments\nextractlongcontiguoussequences.\nIn this section, we present BERT fine-tuning re-\n3.2 Fine-tuningBERT sultson11NLPtasks.\nFine-tuning is straightforward since the self- 4.1 GLUE\nattention mechanism in the Transformer al-\nThe General Language Understanding Evaluation\nlows BERT to model many downstream tasks\u2014\n(GLUE) benchmark (Wang et al., 2018a) is a col-\nwhether they involve single text or text pairs\u2014by\nlection of diverse natural language understanding\nswapping out the appropriate inputs and outputs.\ntasks. DetaileddescriptionsofGLUEdatasetsare\nFor applications involving text pairs, a common\nincludedinAppendixB.1.\npattern is to independently encode text pairs be-\nTo fine-tune on GLUE, we represent the input\nfore applying bidirectional cross attention, such\nsequence (for single sentence or sentence pairs)\nas Parikh et al. (2016); Seo et al. (2017). BERT\nas described in Section 3, and use the final hid-\ninsteadusestheself-attentionmechanismtounify\nden vector C \u2208 RH corresponding to the first\nthese two stages, as encoding a concatenated text\ninput token ([CLS]) as the aggregate representa-\npair with self-attention effectively includes bidi-\ntion. The only new parameters introduced during\nrectionalcrossattentionbetweentwosentences.\nfine-tuning are classification layer weights W \u2208\nFor each task, we simply plug in the task- RK\u00d7H,whereKisthenumberoflabels. Wecom-\nspecific inputs and outputs into BERT and fine-\npute a standard classification loss with C and W,\ntune all the parameters end-to-end. At the in-\ni.e.,log(softmax(CWT)).\nput, sentence A and sentence B from pre-training\nare analogous to (1) sentence pairs in paraphras-\n7Forexample,theBERTSQuADmodelcanbetrainedin\naround30minutesonasingleCloudTPUtoachieveaDev\ning,(2)hypothesis-premisepairsinentailment,(3)\nF1scoreof91.0%.\nquestion-passagepairsinquestionanswering,and 8See(10)inhttps://gluebenchmark.com/faq.\nSystem MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE Average\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k -\nPre-OpenAISOTA 80.6/80.1 66.1 82.3 93.2 35.0 81.0 86.0 61.7 74.0\nBiLSTM+ELMo+Attn 76.4/76.1 64.8 79.8 90.4 36.0 73.3 84.9 56.8 71.0\nOpenAIGPT 82.1/81.4 70.3 87.4 91.3 45.4 80.0 82.3 56.0 75.1\nBERT 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4 79.6\nBASE\nBERT 86.7/85.9 72.1 92.7 94.9 60.5 86.5 89.3 70.1 82.1\nLARGE\nTable1: GLUETestresults, scoredbytheevaluationserver(https://gluebenchmark.com/leaderboard).\nThenumberbeloweachtaskdenotesthenumberoftrainingexamples. The\u201cAverage\u201dcolumnisslightlydifferent\nthantheofficialGLUEscore,sinceweexcludetheproblematicWNLIset.8 BERTandOpenAIGPTaresingle-\nmodel,singletask. F1scoresarereportedforQQPandMRPC,SpearmancorrelationsarereportedforSTS-B,and\naccuracyscoresarereportedfortheothertasks. WeexcludeentriesthatuseBERTasoneoftheircomponents.\nWe use a batch size of 32 and fine-tune for 3 Wikipedia containing the answer, the task is to\nepochsoverthedataforallGLUEtasks. Foreach predicttheanswertextspaninthepassage.\ntask,weselectedthebestfine-tuninglearningrate As shown in Figure 1, in the question answer-\n(among5e-5,4e-5,3e-5,and2e-5)ontheDevset. ing task, we represent the input question and pas-\nAdditionally,forBERT LARGE wefoundthatfine- sage as a single packed sequence, with the ques-\ntuning was sometimes unstable on small datasets, tionusingtheAembeddingandthepassageusing\nsoweranseveralrandomrestartsandselectedthe the B embedding. We only introduce a start vec-\nbest model on the Dev set. With random restarts, tor S \u2208 RH and an end vector E \u2208 RH during\nwe use the same pre-trained checkpoint but per- fine-tuning. The probability of word i being the\nform different fine-tuning data shuffling and clas- startoftheanswerspaniscomputedasadotprod-\nsifierlayerinitialization.9 uct between T and S followed by a softmax over\ni\nResults are presented in Table 1. Both all of the words in the paragraph: P = eS\u00b7Ti .\ni (cid:80) eS\u00b7Tj\nBERT and BERT outperform all sys- j\nBASE LARGE The analogous formula is used for the end of the\ntemsonalltasksbyasubstantialmargin,obtaining\nanswer span. The score of a candidate span from\n4.5% and 7.0% respective average accuracy im-\npositionitopositionj isdefinedasS\u00b7T +E\u00b7T ,\ni j\nprovementoverthepriorstateoftheart. Notethat\nand the maximum scoring span where j \u2265 i is\nBERT and OpenAI GPT are nearly identical\nBASE used as a prediction. The training objective is the\nin terms of model architecture apart from the at-\nsum of the log-likelihoods of the correct start and\ntention masking. For the largest and most widely\nend positions. We fine-tune for 3 epochs with a\nreportedGLUEtask,MNLI,BERTobtainsa4.6%\nlearningrateof5e-5andabatchsizeof32.\nabsolute accuracy improvement. On the official\nTable 2 shows top leaderboard entries as well\nGLUEleaderboard10,BERT obtainsascore\nLARGE\nas results from top published systems (Seo et al.,\nof80.5,comparedtoOpenAIGPT,whichobtains\n2017; Clark and Gardner, 2018; Peters et al.,\n72.8asofthedateofwriting.\n2018a; Hu et al., 2018). The top results from the\nWe find that BERT significantly outper-\nLARGE SQuADleaderboarddonothaveup-to-datepublic\nformsBERT acrossalltasks,especiallythose\nBASE systemdescriptionsavailable,11andareallowedto\nwith very little training data. The effect of model\nuse any public data when training their systems.\nsizeisexploredmorethoroughlyinSection5.2.\nWe therefore use modest data augmentation in\noursystembyfirstfine-tuningonTriviaQA(Joshi\n4.2 SQuADv1.1\netal.,2017)beforfine-tuningonSQuAD.\nThe Stanford Question Answering Dataset\nOurbestperformingsystemoutperformsthetop\n(SQuAD v1.1) is a collection of 100k crowd-\nleaderboardsystemby+1.5F1inensemblingand\nsourced question/answer pairs (Rajpurkar et al.,\n+1.3 F1 as a single system. In fact, our single\n2016). Given a question and a passage from\nBERT model outperforms the top ensemble sys-\n9TheGLUEdatasetdistributiondoesnotincludetheTest tem in terms of F1 score. Without TriviaQA fine-\nlabels, and we only made a single GLUE evaluation server\nsubmissionforeachofBERT BASE andBERT LARGE . 11QANet is described in Yu et al. (2018), but the system\n10https://gluebenchmark.com/leaderboard hasimprovedsubstantiallyafterpublication.\nSystem Dev Test System Dev Test\nEM F1 EM F1\nESIM+GloVe 51.9 52.7\nTopLeaderboardSystems(Dec10th,2018) ESIM+ELMo 59.1 59.2\nHuman - - 82.3 91.2 OpenAIGPT - 78.0\n#1Ensemble-nlnet - - 86.0 91.7\nBERT 81.6 -\n#2Ensemble-QANet - - 84.5 90.5 BASE\nBERT 86.6 86.3\nLARGE\nPublished\nHuman(expert)\u2020 - 85.0\nBiDAF+ELMo(Single) - 85.6 - 85.8\nR.M.Reader(Ensemble) 81.2 87.9 82.3 88.5 Human(5annotations)\u2020 - 88.0\nOurs\nTable4: SWAGDevandTestaccuracies. \u2020Humanper-\nBERT (Single) 80.8 88.5 - -\nBASE\nBERT (Single) 84.1 90.9 - - formanceismeasuredwith100samples,asreportedin\nLARGE\nBERT (Ensemble) 85.8 91.8 - - theSWAGpaper.\nLARGE\nBERT (Sgl.+TriviaQA) 84.2 91.1 85.1 91.8\nLARGE\nBERT (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\nLARGE\ns\u02c6 = max S\u00b7T +E\u00b7T . We predict a non-null\ni,j j\u2265i i j\nTable 2: SQuAD 1.1 results. The BERT ensemble\nanswer when s\u02c6 > s +\u03c4, where the thresh-\ni,j null\nis 7x systems which use different pre-training check-\nold \u03c4 is selected on the dev set to maximize F1.\npointsandfine-tuningseeds.\nWe did not use TriviaQA data for this model. We\nfine-tunedfor2epochswithalearningrateof5e-5\nSystem Dev Test\nandabatchsizeof48.\nEM F1 EM F1\nThe results compared to prior leaderboard en-\nTopLeaderboardSystems(Dec10th,2018)\nHuman 86.3 89.0 86.9 89.5 tries and top published work (Sun et al., 2018;\n#1Single-MIR-MRC(F-Net) - - 74.8 78.0 Wangetal.,2018b)areshowninTable3,exclud-\n#2Single-nlnet - - 74.2 77.1\ning systems that use BERT as one of their com-\nPublished\nponents. Weobservea+5.1F1improvementover\nunet(Ensemble) - - 71.4 74.9\nSLQA+(Single) - 71.4 74.4 thepreviousbestsystem.\nOurs\n4.4 SWAG\nBERT (Single) 78.7 81.9 80.0 83.1\nLARGE\nThe Situations With Adversarial Generations\nTable 3: SQuAD 2.0 results. We exclude entries that (SWAG)datasetcontains113ksentence-paircom-\nuseBERTasoneoftheircomponents. pletionexamplesthatevaluategroundedcommon-\nsenseinference(Zellersetal.,2018). Givenasen-\ntence,thetaskistochoosethemostplausiblecon-\ntuning data, we only lose 0.1-0.4 F1, still outper-\ntinuationamongfourchoices.\nformingallexistingsystemsbyawidemargin.12\nWhen fine-tuning on the SWAG dataset, we\n4.3 SQuADv2.0 construct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nThe SQuAD 2.0 task extends the SQuAD 1.1\nA) and a possible continuation (sentence B). The\nproblem definition by allowing for the possibility\nonly task-specific parameters introduced is a vec-\nthat no short answer exists in the provided para-\ntorwhosedotproductwiththe[CLS]tokenrep-\ngraph,makingtheproblemmorerealistic.\nresentation C denotes a score for each choice\nWeuseasimpleapproachtoextendtheSQuAD\nwhichisnormalizedwithasoftmaxlayer.\nv1.1 BERT model for this task. We treat ques-\nWe fine-tune the model for 3 epochs with a\ntions that do not have an answer as having an an-\nlearning rate of 2e-5 and a batch size of 16. Re-\nswer span with start and end at the [CLS] to-\nsults are presented in Table 4. BERT out-\nken. The probability space for the start and end LARGE\nperforms the authors\u2019 baseline ESIM+ELMo sys-\nanswer span positions is extended to include the\ntemby+27.1%andOpenAIGPTby8.3%.\nposition of the [CLS] token. For prediction, we\ncomparethescoreoftheno-answerspan: s =\nnull 5 AblationStudies\nS\u00b7C +E\u00b7C tothescoreofthebestnon-nullspan\nIn this section, we perform ablation experiments\n12TheTriviaQAdataweusedconsistsofparagraphsfrom\noveranumberoffacetsofBERTinordertobetter\nTriviaQA-Wikiformedofthefirst400tokensindocuments,\nthatcontainatleastoneoftheprovidedpossibleanswers. understand their relative importance. Additional\nDevSet results are still far worse than those of the pre-\nTasks MNLI-m QNLI MRPC SST-2 SQuAD trained bidirectional models. The BiLSTM hurts\n(Acc) (Acc) (Acc) (Acc) (F1)\nperformanceontheGLUEtasks.\nBERT 84.4 88.4 86.7 92.7 88.5\nBASE We recognize that it would also be possible to\nNoNSP 83.9 84.9 86.5 92.6 87.9\nLTR&NoNSP 82.1 84.3 77.5 92.1 77.8 trainseparateLTRandRTLmodelsandrepresent\n+BiLSTM 82.1 84.1 75.7 91.6 84.9 each token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nTable5: Ablationoverthepre-trainingtasksusingthe\nexpensive as a single bidirectional model; (b) this\nBERT architecture. \u201cNo NSP\u201d is trained without\nBASE\nis non-intuitive for tasks like QA, since the RTL\nthenextsentencepredictiontask. \u201cLTR&NoNSP\u201dis\ntrainedasaleft-to-rightLMwithoutthenextsentence model would not be able to condition the answer\nprediction,likeOpenAIGPT.\u201c+BiLSTM\u201daddsaran- onthequestion; (c)thisitisstrictlylesspowerful\ndomly initialized BiLSTM on top of the \u201cLTR + No than a deep bidirectional model, since it can use\nNSP\u201dmodelduringfine-tuning. bothleftandrightcontextateverylayer.\n5.2 EffectofModelSize\nablationstudiescanbefoundinAppendixC.\nInthissection,weexploretheeffectofmodelsize\n5.1 EffectofPre-trainingTasks onfine-tuningtaskaccuracy. Wetrainedanumber\nofBERTmodelswithadifferingnumberoflayers,\nWe demonstrate the importance of the deep bidi-\nhiddenunits,andattentionheads,whileotherwise\nrectionality of BERT by evaluating two pre-\nusing the same hyperparameters and training pro-\ntraining objectives using exactly the same pre-\ncedureasdescribedpreviously.\ntraining data, fine-tuning scheme, and hyperpa-\nResults on selected GLUE tasks are shown in\nrametersasBERT :\nBASE\nTable 6. In this table, we report the average Dev\nNo NSP: A bidirectional model which is trained Setaccuracyfrom5randomrestartsoffine-tuning.\nusing the \u201cmasked LM\u201d (MLM) but without the We can see that larger models lead to a strict ac-\n\u201cnextsentenceprediction\u201d(NSP)task. curacyimprovementacrossallfourdatasets, even\nLTR&NoNSP:Aleft-context-onlymodelwhich for MRPC which only has 3,600 labeled train-\nis trained using a standard Left-to-Right (LTR) ing examples, and is substantially different from\nLM,ratherthananMLM.Theleft-onlyconstraint the pre-training tasks. It is also perhaps surpris-\nwasalsoappliedatfine-tuning,becauseremoving ing that we are able to achieve such significant\nit introduced a pre-train/fine-tune mismatch that improvements on top of models which are al-\ndegraded downstream performance. Additionally, readyquitelargerelativetotheexistingliterature.\nthis model was pre-trained without the NSP task. For example, the largest Transformer explored in\nThis is directly comparable to OpenAI GPT, but Vaswani et al. (2017) is (L=6, H=1024, A=16)\nusing our larger training dataset, our input repre- with 100M parameters for the encoder, and the\nsentation,andourfine-tuningscheme. largestTransformerwehavefoundintheliterature\nWefirstexaminetheimpactbroughtbytheNSP is (L=64, H=512, A=2) with 235M parameters\ntask. In Table 5, we show that removing NSP (Al-Rfou et al., 2018). By contrast, BERT\nBASE\nhurts performance significantly on QNLI, MNLI, contains 110M parameters and BERT con-\nLARGE\nand SQuAD 1.1. Next, we evaluate the impact tains340Mparameters.\nof training bidirectional representations by com- It has long been known that increasing the\nparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model size will lead to continual improvements\nmodelperformsworsethantheMLMmodelonall on large-scale tasks such as machine translation\ntasks,withlargedropsonMRPCandSQuAD. and language modeling, which is demonstrated\nFor SQuAD it is intuitively clear that a LTR by the LM perplexity of held-out training data\nmodel will perform poorly at token predictions, shown in Table 6. However, we believe that\nsince the token-level hidden states have no right- this is the first work to demonstrate convinc-\nside context. In order to make a good faith at- ingly that scaling to extreme model sizes also\ntempt at strengthening the LTR system, we added leads to large improvements on very small scale\narandomlyinitializedBiLSTMontop. Thisdoes tasks, provided that the model has been suffi-\nsignificantly improve results on SQuAD, but the cientlypre-trained. Petersetal.(2018b)presented\nmixed results on the downstream task impact of System DevF1 TestF1\nincreasing the pre-trained bi-LM size from two\nELMo(Petersetal.,2018a) 95.7 92.2\nto four layers and Melamud et al. (2016) men- CVT(Clarketal.,2018) - 92.6\nCSE(Akbiketal.,2018) - 93.1\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing Fine-tuningapproach\nBERT 96.6 92.8\nfurther to 1,000 did not bring further improve- LARGE\nBERT 96.4 92.4\nBASE\nments. Both of these prior works used a feature-\nFeature-basedapproach(BERT )\nBASE\nbased approach \u2014 we hypothesize that when the Embeddings 91.0 -\nmodel is fine-tuned directly on the downstream Second-to-LastHidden 95.6 -\nLastHidden 94.9 -\ntasks and uses only a very small number of ran-\nWeightedSumLastFourHidden 95.9 -\ndomly initialized additional parameters, the task- ConcatLastFourHidden 96.1 -\nspecific models can benefit from the larger, more WeightedSumAll12Layers 95.5 -\nexpressive pre-trained representations even when\nTable 7: CoNLL-2003 Named Entity Recognition re-\ndownstreamtaskdataisverysmall.\nsults. Hyperparameters were selected using the Dev\nset.ThereportedDevandTestscoresareaveragedover\n5.3 Feature-basedApproachwithBERT\n5randomrestartsusingthosehyperparameters.\nAlloftheBERTresultspresentedsofarhaveused\nthe fine-tuning approach, where a simple classifi-\ncationlayerisaddedtothepre-trainedmodel,and layer in the output. We use the representation of\nall parameters are jointly fine-tuned on a down- the first sub-token as the input to the token-level\nstreamtask. However,thefeature-basedapproach, classifierovertheNERlabelset.\nwhere fixed features are extracted from the pre-\nToablatethefine-tuningapproach,weapplythe\ntrained model, has certain advantages. First, not\nfeature-based approach by extracting the activa-\nall tasks can be easily represented by a Trans-\ntions from one or more layers without fine-tuning\nformerencoderarchitecture,andthereforerequire\nany parameters of BERT. These contextual em-\na task-specific model architecture to be added.\nbeddings are used as input to a randomly initial-\nSecond, there are major computational benefits\nized two-layer 768-dimensional BiLSTM before\ntopre-computeanexpensiverepresentationofthe\ntheclassificationlayer.\ntrainingdataonceandthenrunmanyexperiments\nResults are presented in Table 7. BERT\nwithcheapermodelsontopofthisrepresentation. LARGE\nperformscompetitivelywithstate-of-the-artmeth-\nInthissection,wecomparethetwoapproaches\nods. Thebestperformingmethodconcatenatesthe\nby applying BERT to the CoNLL-2003 Named\ntokenrepresentationsfromthetopfourhiddenlay-\nEntity Recognition (NER) task (Tjong Kim Sang\ners of the pre-trained Transformer, which is only\nandDeMeulder,2003). IntheinputtoBERT,we\n0.3 F1 behind fine-tuning the entire model. This\nuse a case-preserving WordPiece model, and we\ndemonstratesthatBERTiseffectiveforbothfine-\ninclude the maximal document context provided\ntuningandfeature-basedapproaches.\nby the data. Following standard practice, we for-\nmulatethisasataggingtaskbutdonotuseaCRF\n6 Conclusion\nHyperparams DevSetAccuracy\nRecent empirical improvements due to transfer\n#L #H #A LM(ppl) MNLI-m MRPC SST-2\nlearningwithlanguagemodelshavedemonstrated\n3 768 12 5.84 77.9 79.8 88.4\n6 768 3 5.24 80.6 82.2 90.7 that rich, unsupervised pre-training is an integral\n6 768 12 4.68 81.9 84.8 91.3 part of many language understanding systems. In\n12 768 12 3.99 84.4 86.7 92.9\nparticular, these results enable even low-resource\n12 1024 16 3.54 85.7 86.9 93.3\n24 1024 16 3.23 86.6 87.8 93.7 taskstobenefitfromdeepunidirectionalarchitec-\ntures. Our major contribution is further general-\nTable 6: Ablation over BERT model size. #L = the izingthesefindingstodeepbidirectionalarchitec-\nnumberoflayers;#H=hiddensize;#A=numberofat- tures,allowingthesamepre-trainedmodeltosuc-\ntentionheads.\u201cLM(ppl)\u201disthemaskedLMperplexity cessfullytackleabroadsetofNLPtasks.\nofheld-outtrainingdata.\nReferences KevinClark,Minh-ThangLuong,ChristopherDMan-\nning, and Quoc Le. 2018. Semi-supervised se-\nAlan Akbik, Duncan Blythe, and Roland Vollgraf. quence modeling with cross-view training. In Pro-\n2018. Contextual string embeddings for sequence ceedingsofthe2018ConferenceonEmpiricalMeth-\nlabeling. In Proceedings of the 27th International ods in Natural Language Processing, pages 1914\u2013\nConference on Computational Linguistics, pages 1925.\n1638\u20131649.\nRonan Collobert and Jason Weston. 2008. A unified\nRamiAl-Rfou, DokookChoe, NoahConstant, Mandy architecture for natural language processing: Deep\nGuo, and Llion Jones. 2018. Character-level lan- neural networks with multitask learning. In Pro-\nguage modeling with deeper self-attention. arXiv ceedings of the 25th international conference on\npreprintarXiv:1808.04444. Machinelearning,pages160\u2013167.ACM.\nRieKubotaAndoandTongZhang.2005. Aframework\nAlexisConneau,DouweKiela,HolgerSchwenk,Lo\u00a8\u0131c\nforlearningpredictivestructuresfrommultipletasks Barrault, and Antoine Bordes. 2017. Supervised\nand unlabeled data. Journal of Machine Learning learning of universal sentence representations from\nResearch,6(Nov):1817\u20131853. natural language inference data. In Proceedings of\nthe2017ConferenceonEmpiricalMethodsinNat-\nural Language Processing, pages 670\u2013680, Copen-\nLuisa Bentivogli, Bernardo Magnini, Ido Dagan,\nhagen, Denmark. Association for Computational\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nLinguistics.\nThe fifth PASCAL recognizing textual entailment\nchallenge. InTAC.NIST.\nAndrewMDaiandQuocVLe.2015. Semi-supervised\nsequence learning. In Advances in neural informa-\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\ntionprocessingsystems,pages3079\u20133087.\n2006. Domainadaptationwithstructuralcorrespon-\ndencelearning. InProceedingsofthe2006confer-\nJ.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-\nenceonempiricalmethodsinnaturallanguagepro-\nFei. 2009. ImageNet: A Large-Scale Hierarchical\ncessing, pages 120\u2013128. Association for Computa-\nImageDatabase. InCVPR09.\ntionalLinguistics.\nWilliamBDolanandChrisBrockett.2005. Automati-\nSamuelR.Bowman,GaborAngeli,ChristopherPotts, callyconstructingacorpusofsententialparaphrases.\nand Christopher D. Manning. 2015. A large anno- InProceedingsoftheThirdInternationalWorkshop\ntatedcorpusforlearningnaturallanguageinference. onParaphrasing(IWP2005).\nInEMNLP.AssociationforComputationalLinguis-\ntics. William Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Bettertextgenerationviafillingin\nPeter F Brown, Peter V Desouza, Robert L Mercer, the . arXivpreprintarXiv:1801.07736.\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural language. Dan Hendrycks and Kevin Gimpel. 2016. Bridging\nComputationallinguistics,18(4):467\u2013479. nonlinearitiesandstochasticregularizerswithgaus-\nsianerrorlinearunits. CoRR,abs/1606.08415.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nFelixHill,KyunghyunCho,andAnnaKorhonen.2016.\nGazpio, and Lucia Specia. 2017. Semeval-2017\nLearning distributed representations of sentences\ntask1: Semantictextualsimilaritymultilingualand\nfrom unlabelled data. In Proceedings of the 2016\ncrosslingual focused evaluation. In Proceedings\nConference of the North American Chapter of the\nof the 11th International Workshop on Semantic\nAssociationforComputationalLinguistics: Human\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nLanguage Technologies. Association for Computa-\nver, Canada. Association for Computational Lin-\ntionalLinguistics.\nguistics.\nJeremyHowardandSebastianRuder.2018. Universal\nCiprianChelba,TomasMikolov,MikeSchuster,QiGe,\nlanguagemodelfine-tuningfortextclassification. In\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nACL.AssociationforComputationalLinguistics.\nson.2013. Onebillionwordbenchmarkformeasur-\ningprogressinstatisticallanguagemodeling. arXiv\nMinghaoHu,YuxingPeng,ZhenHuang,XipengQiu,\npreprintarXiv:1312.3005.\nFuru Wei, and Ming Zhou. 2018. Reinforced\nmnemonic reader for machine reading comprehen-\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. sion. InIJCAI.\nQuoraquestionpairs.\nYacine Jernite, Samuel R. Bowman, and David Son-\nChristopher Clark and Matt Gardner. 2018. Simple tag. 2017. Discourse-based objectives for fast un-\nand effective multi-paragraph reading comprehen- supervisedsentencerepresentationlearning. CoRR,\nsion. InACL. abs/1705.00557.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Matthew Peters, Mark Neumann, Luke Zettlemoyer,\nZettlemoyer.2017. Triviaqa: Alargescaledistantly and Wen-tau Yih. 2018b. Dissecting contextual\nsupervisedchallengedatasetforreadingcomprehen- wordembeddings: Architectureandrepresentation.\nsion. InACL. In Proceedings of the 2018 Conference on Empiri-\ncalMethodsinNaturalLanguageProcessing,pages\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, 1499\u20131509.\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In AlecRadford,KarthikNarasimhan,TimSalimans,and\nAdvancesinneuralinformationprocessingsystems, Ilya Sutskever. 2018. Improving language under-\npages3294\u20133302. standing with unsupervised learning. Technical re-\nport,OpenAI.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter- PranavRajpurkar,JianZhang,KonstantinLopyrev,and\nnational Conference on Machine Learning, pages Percy Liang. 2016. Squad: 100,000+ questions for\n1188\u20131196. machine comprehension of text. In Proceedings of\nthe2016ConferenceonEmpiricalMethodsinNat-\nHector J Levesque, Ernest Davis, and Leora Morgen-\nuralLanguageProcessing,pages2383\u20132392.\nstern. 2011. The winograd schema challenge. In\nAaai spring symposium: Logical formalizations of Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\ncommonsensereasoning,volume46,page47. Hannaneh Hajishirzi. 2017. Bidirectional attention\nflowformachinecomprehension. InICLR.\nLajanugen Logeswaran and Honglak Lee. 2018. An\nefficient framework for learning sentence represen- Richard Socher, Alex Perelygin, Jean Wu, Jason\ntations. In International Conference on Learning Chuang, Christopher D Manning, Andrew Ng, and\nRepresentations.\nChristopher Potts. 2013. Recursive deep models\nforsemanticcompositionalityoverasentimenttree-\nBryanMcCann,JamesBradbury,CaimingXiong,and\nbank. In Proceedings of the 2013 conference on\nRichardSocher.2017. Learnedintranslation: Con-\nempirical methods in natural language processing,\ntextualizedwordvectors. InNIPS.\npages1631\u20131642.\nOren Melamud, Jacob Goldberger, and Ido Dagan.\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\n2016. context2vec: Learning generic context em-\n2018. U-net: Machine reading comprehension\nbeddingwithbidirectionalLSTM. InCoNLL.\nwith unanswerable questions. arXiv preprint\nTomasMikolov,IlyaSutskever,KaiChen,GregSCor- arXiv:1810.06638.\nrado, and Jeff Dean. 2013. Distributed representa-\nWilson L Taylor. 1953. Cloze procedure: A new\ntionsofwordsandphrasesandtheircompositional-\ntoolformeasuringreadability. JournalismBulletin,\nity. In Advances in Neural Information Processing\n30(4):415\u2013433.\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.\nErik F Tjong Kim Sang and Fien De Meulder.\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal- 2003. Introduction to the conll-2003 shared task:\nable hierarchical distributed language model. In Language-independentnamedentityrecognition. In\nD. Koller, D. Schuurmans, Y. Bengio, and L. Bot-\nCoNLL.\ntou, editors, Advances in Neural Information Pro-\nJosephTurian,LevRatinov,andYoshuaBengio.2010.\ncessing Systems 21, pages 1081\u20131088. Curran As-\nWordrepresentations: Asimpleandgeneralmethod\nsociates,Inc.\nforsemi-supervisedlearning. InProceedingsofthe\nAnkur P Parikh, Oscar Ta\u00a8ckstro\u00a8m, Dipanjan Das, and 48thAnnualMeetingoftheAssociationforCompu-\nJakob Uszkoreit. 2016. A decomposable attention tationalLinguistics,ACL\u201910,pages384\u2013394.\nmodelfornaturallanguageinference. InEMNLP.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nJeffrey Pennington, Richard Socher, and Christo- Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\npher D. Manning. 2014. Glove: Global vectors for Kaiser, and Illia Polosukhin. 2017. Attention is all\nwordrepresentation. InEmpiricalMethodsinNat- you need. In Advances in Neural Information Pro-\nural Language Processing (EMNLP), pages 1532\u2013 cessingSystems,pages6000\u20136010.\n1543.\nPascalVincent,HugoLarochelle,YoshuaBengio,and\nMatthew Peters, Waleed Ammar, Chandra Bhagavat- Pierre-Antoine Manzagol. 2008. Extracting and\nula, and Russell Power. 2017. Semi-supervised se- composing robust features with denoising autoen-\nquencetaggingwithbidirectionallanguagemodels. coders. In Proceedings of the 25th international\nInACL. conferenceonMachinelearning,pages1096\u20131103.\nACM.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke Alex Wang, Amanpreet Singh, Julian Michael, Fe-\nZettlemoyer.2018a. Deepcontextualizedwordrep- lix Hill, Omer Levy, and Samuel Bowman. 2018a.\nresentations. InNAACL. Glue:Amulti-taskbenchmarkandanalysisplatform\nfornaturallanguageunderstanding. InProceedings \u2022 Additional details for our experiments are\nof the 2018 EMNLP Workshop BlackboxNLP: An- presentedinAppendixB;and\nalyzing and Interpreting Neural Networks for NLP,\npages353\u2013355.\n\u2022 Additional ablation studies are presented in\nAppendixC.\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\ngranularity hierarchical attention fusion networks We present additional ablation studies for\nforreadingcomprehensionandquestionanswering.\nBERTincluding:\nInProceedingsofthe56thAnnualMeetingoftheAs-\nsociationforComputationalLinguistics(Volume1:\n\u2013 EffectofNumberofTrainingSteps;and\nLong Papers). Association for Computational Lin-\nguistics. \u2013 Ablation for Different Masking Proce-\ndures.\nAlexWarstadt,AmanpreetSingh,andSamuelRBow-\nman. 2018. Neural network acceptability judg- A AdditionalDetailsforBERT\nments. arXivpreprintarXiv:1805.12471.\nA.1 IllustrationofthePre-trainingTasks\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018. A broad-coverage challenge corpus We provide examples of the pre-training tasks in\nfor sentence understanding through inference. In thefollowing.\nNAACL.\nMasked LM and the Masking Procedure As-\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nsuming the unlabeled sentence is my dog is\nLe, Mohammad Norouzi, Wolfgang Macherey,\nhairy,andduringtherandommaskingprocedure\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google\u2019s neural ma- we chose the 4-th token (which corresponding to\nchinetranslationsystem: Bridgingthegapbetween hairy), our masking procedure can be further il-\nhuman and machine translation. arXiv preprint lustratedby\narXiv:1609.08144.\n\u2022 80% of the time: Replace the word with the\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\n[MASK] token, e.g., my dog is hairy \u2192\nLipson.2014. Howtransferablearefeaturesindeep\nneuralnetworks? InAdvancesinneuralinformation my dog is [MASK]\nprocessingsystems,pages3320\u20133328.\n\u2022 10% of the time: Replace the word with a\nAdamsWeiYu,DavidDohan,Minh-ThangLuong,Rui randomword,e.g.,my dog is hairy \u2192 my\nZhao,KaiChen,MohammadNorouzi,andQuocV\ndog is apple\nLe. 2018. QANet: Combining local convolution\nwith global self-attention for reading comprehen-\n\u2022 10% of the time: Keep the word un-\nsion. InICLR.\nchanged,e.g.,my dog is hairy \u2192 my dog\nRowanZellers,YonatanBisk,RoySchwartz,andYejin is hairy. The purpose of this is to bias the\nChoi.2018. Swag: Alarge-scaleadversarialdataset\nrepresentation towards the actual observed\nfor grounded commonsense inference. In Proceed-\nword.\nings of the 2018 Conference on Empirical Methods\ninNaturalLanguageProcessing(EMNLP).\nThe advantage of this procedure is that the\nYukunZhu,RyanKiros,RichZemel,RuslanSalakhut- Transformer encoder does not know which words\ndinov,RaquelUrtasun,AntonioTorralba,andSanja it will be asked to predict or which have been re-\nFidler.2015. Aligningbooksandmovies: Towards\nplaced by random words, so it is forced to keep\nstory-like visual explanations by watching movies\na distributional contextual representation of ev-\nand reading books. In Proceedings of the IEEE\ninternational conference on computer vision, pages ery input token. Additionally, because random\n19\u201327. replacement only occurs for 1.5% of all tokens\n(i.e., 10% of 15%), this does not seem to harm\nAppendixfor\u201cBERT:Pre-trainingof\nthemodel\u2019slanguageunderstandingcapability. In\nDeepBidirectionalTransformersfor\nSection C.2, we evaluate the impact this proce-\nLanguageUnderstanding\u201d\ndure.\nWeorganizetheappendixintothreesections: Comparedtostandardlangaugemodeltraining,\nthe maskedLM onlymake predictions on15% of\n\u2022 Additional implementation details for BERT tokens in each batch, which suggests that more\narepresentedinAppendixA; pre-training steps may be required for the model\nBERT (Ours) OpenAI GPT ELMo\nT1 T2 ... TN T1 T2 ... TN T1 T2 ... TN\nTrm Trm ... Trm Trm Trm ... Trm\nLstm Lstm ... Lstm Lstm Lstm ... Lstm\nTrm Trm ... Trm Trm Trm ... Trm\nLstm Lstm ... Lstm Lstm Lstm ... Lstm\nE1 E2 ... EN E1 E2 ... EN E1 E2 ... EN\nFigure3: Differencesinpre-trainingmodelarchitectures. BERTusesabidirectionalTransformer. OpenAIGPT\nusesaleft-to-rightTransformer. ELMousestheconcatenationofindependentlytrainedleft-to-rightandright-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAIGPTarefine-tuningapproaches,whileELMoisafeature-basedapproach.\nto converge. In Section C.1 we demonstrate that epochs over the 3.3 billion word corpus. We\nMLMdoesconvergemarginallyslowerthanaleft- use Adam with learning rate of 1e-4, \u03b2 = 0.9,\n1\nto-right model (which predicts every token), but \u03b2 = 0.999, L2 weight decay of 0.01, learning\n2\nthe empirical improvements of the MLM model ratewarmupoverthefirst10,000steps,andlinear\nfaroutweightheincreasedtrainingcost. decayofthelearningrate. Weuseadropoutprob-\nability of 0.1 on all layers. We use a gelu acti-\nNext Sentence Prediction The next sentence\nvation (Hendrycks and Gimpel, 2016) rather than\nprediction task can be illustrated in the following\nthe standard relu, following OpenAI GPT. The\nexamples.\ntraining loss is the sum of the mean masked LM\nlikelihood and the mean next sentence prediction\nInput = [CLS] the man went to [MASK] store [SEP]\nlikelihood.\nhe bought a gallon [MASK] milk [SEP] Training of BERT was performed on 4\nBASE\nLabel = IsNext Cloud TPUs in Pod configuration (16 TPU chips\ntotal).13 Training of BERT was performed\nLARGE\non16CloudTPUs(64TPUchipstotal). Eachpre-\nInput = [CLS] the man [MASK] to the store [SEP]\ntrainingtook4daystocomplete.\npenguin [MASK] are flight ##less birds [SEP]\nLongersequencesaredisproportionatelyexpen-\nLabel = NotNext sivebecauseattentionisquadratictothesequence\nlength. To speed up pretraing in our experiments,\nA.2 Pre-trainingProcedure\nwe pre-train the model with sequence length of\nTogenerateeachtraininginputsequence,wesam- 128 for 90% of the steps. Then, we train the rest\nple two spans of text from the corpus, which we 10% of the steps of sequence of 512 to learn the\nrefer to as \u201csentences\u201d even though they are typ- positionalembeddings.\nically much longer than single sentences (but can\nbe shorter also). The first sentence receives the A A.3 Fine-tuningProcedure\nembedding and the second receives the B embed- For fine-tuning, most model hyperparameters are\nding. 50%ofthetimeBistheactualnextsentence the same as in pre-training, with the exception of\nthat follows A and 50% of the time it is a random the batch size, learning rate, and number of train-\nsentence,whichisdoneforthe\u201cnextsentencepre- ing epochs. The dropout probability was always\ndiction\u201dtask. Theyaresampledsuchthatthecom- kept at 0.1. The optimal hyperparameter values\nbinedlengthis\u2264512tokens. TheLMmaskingis aretask-specific,butwefoundthefollowingrange\napplied after WordPiece tokenization with a uni- ofpossiblevaluestoworkwellacrossalltasks:\nformmaskingrateof15%,andnospecialconsid-\nerationgiventopartialwordpieces. \u2022 Batchsize: 16,32\nWetrainwithbatchsizeof256sequences(256\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\nsequences * 512 tokens = 128,000 tokens/batch)\nTPU-now-offers-preemptible-pricing-and-global-\nfor 1,000,000 steps, which is approximately 40 availability.html\n\u2022 Learningrate(Adam): 5e-5,3e-5,2e-5 Toisolatetheeffectofthesedifferences,weper-\n\u2022 Numberofepochs: 2,3,4 form ablation experiments in Section 5.1 which\ndemonstratethatthemajorityoftheimprovements\nWe also observed that large data sets (e.g., areinfactcomingfromthetwopre-trainingtasks\n100k+ labeled training examples) were far less andthebidirectionalitytheyenable.\nsensitivetohyperparameterchoicethansmalldata\nsets. Fine-tuningistypicallyveryfast,soitisrea- A.5 IllustrationsofFine-tuningonDifferent\nsonable to simply run an exhaustive search over Tasks\nthe above parameters and choose the model that The illustration of fine-tuning BERT on different\nperformsbestonthedevelopmentset. tasks can be seen in Figure 4. Our task-specific\nmodels are formed by incorporating BERT with\nA.4 ComparisonofBERT,ELMo,and\none additional output layer, so a minimal num-\nOpenAIGPT\nberofparametersneedtobelearnedfromscratch.\nHere we studies the differences in recent popular Among the tasks, (a) and (b) are sequence-level\nrepresentation learning models including ELMo, tasks while (c) and (d) are token-level tasks. In\nOpenAI GPT and BERT. The comparisons be- the figure, E represents the input embedding, T\ni\ntween the model architectures are shown visually representsthecontextualrepresentationoftokeni,\nin Figure 3. Note that in addition to the architec- [CLS]isthespecialsymbolforclassificationout-\nturedifferences,BERTandOpenAIGPTarefine- put, and [SEP] is the special symbol to separate\ntuningapproaches,whileELMoisafeature-based non-consecutivetokensequences.\napproach.\nB DetailedExperimentalSetup\nThe most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nB.1 DetailedDescriptionsfortheGLUE\nleft-to-right Transformer LM on a large text cor-\nBenchmarkExperiments.\npus. Infact,manyofthedesigndecisionsinBERT\nOur GLUE results in Table1 are obtained\nwere intentionally made to make it as close to\nfrom https://gluebenchmark.com/\nGPTaspossiblesothatthetwomethodscouldbe\nleaderboard and https://blog.\nminimally compared. The core argument of this\nopenai.com/language-unsupervised.\nwork is that the bi-directionality and the two pre-\nThe GLUE benchmark includes the following\ntrainingtaskspresentedinSection3.1accountfor\ndatasets,thedescriptionsofwhichwereoriginally\nthe majority of the empirical improvements, but\nsummarizedinWangetal.(2018a):\nwedonotethatthereareseveralotherdifferences\nbetweenhowBERTandGPTweretrained: MNLI Multi-GenreNaturalLanguageInference\nis a large-scale, crowdsourced entailment classifi-\n\u2022 GPT is trained on the BooksCorpus (800M cationtask(Williamsetal.,2018). Givenapairof\nwords); BERT is trained on the BooksCor- sentences, the goal is to predict whether the sec-\npus (800M words) and Wikipedia (2,500M ond sentence is an entailment, contradiction, or\nwords). neutralwithrespecttothefirstone.\n\u2022 GPT uses a sentence separator ([SEP]) and QQP Quora Question Pairs is a binary classifi-\nclassifier token ([CLS]) which are only in- cation task where the goal is to determine if two\ntroduced at fine-tuning time; BERT learns questions asked on Quora are semantically equiv-\n[SEP], [CLS] and sentence A/B embed- alent(Chenetal.,2018).\ndingsduringpre-training.\nQNLI Question Natural Language Inference is\na version of the Stanford Question Answering\n\u2022 GPT was trained for 1M steps with a batch\nDataset (Rajpurkar et al., 2016) which has been\nsize of 32,000 words; BERT was trained for\nconverted to a binary classification task (Wang\n1Mstepswithabatchsizeof128,000words.\net al., 2018a). The positive examples are (ques-\n\u2022 GPT used the same learning rate of 5e-5 for tion, sentence) pairs which do contain the correct\nallfine-tuningexperiments; BERTchoosesa answer, and the negative examples are (question,\ntask-specific fine-tuning learning rate which sentence) from the same paragraph which do not\nperformsthebestonthedevelopmentset. containtheanswer.\nClass Class\nLabel Label\nC T 1 ... T N T [SEP] T 1 \u2019 ... T M \u2019 C T 1 T 2 ... T N\nBERT BERT\nE[CLS] E 1 ... E N E [SEP] E 1 \u2019 ... E M \u2019 E [CLS] E 1 E 2 ... E N\n[CLS] T 1 ok ... T N ok [SEP] T 1 ok ... T M ok [[CCLLSS]] TTookk 11 Tok 2 ... Tok N\nSentence 1 Sentence 2 Single Sentence\nStart/End Span O B-PER ... O\nC T 1 ... T N T [SEP] T 1 \u2019 ... T M \u2019 C T 1 T 2 ... T N\nBERT BERT\nE[CLS] E 1 ... E N E [SEP] E 1 \u2019 ... E M \u2019 E [CLS] E 1 E 2 ... E N\n[CLS] T 1 ok ... T N ok [SEP] T 1 ok ... T M ok [CLS] Tok 1 Tok 2 ... Tok N\nQuestion Paragraph Single Sentence\nFigure4: IllustrationsofFine-tuningBERTonDifferentTasks.\nSST-2 The Stanford Sentiment Treebank is a for whether the sentences in the pair are semanti-\nbinary single-sentence classification task consist- callyequivalent(DolanandBrockett,2005).\ning of sentences extracted from movie reviews\nRTE Recognizing Textual Entailment is a bi-\nwithhumanannotationsoftheirsentiment(Socher\nnary entailment task similar to MNLI, but with\netal.,2013).\nmuchlesstrainingdata(Bentivoglietal.,2009).14\nCoLA TheCorpusofLinguisticAcceptabilityis\nabinarysingle-sentenceclassificationtask,where WNLI Winograd NLI is a small natural lan-\nthegoalistopredictwhetheranEnglishsentence guage inference dataset (Levesque et al., 2011).\nis linguistically \u201cacceptable\u201d or not (Warstadt The GLUE webpage notes that there are issues\netal.,2018). with the construction of this dataset, 15 and every\ntrainedsystemthat\u2019sbeensubmittedtoGLUEhas\nSTS-B TheSemanticTextualSimilarityBench-\nperformed worse than the 65.1 baseline accuracy\nmark is a collection of sentence pairs drawn from\nof predicting the majority class. We therefore ex-\nnews headlines and other sources (Cer et al.,\nclude this set to be fair to OpenAI GPT. For our\n2017). They were annotated with a score from 1\nGLUE submission, we always predicted the ma-\nto5denotinghowsimilarthetwosentencesarein\ntermsofsemanticmeaning. 14Note that we only report single-task fine-tuning results\ninthispaper. Amultitaskfine-tuningapproachcouldpoten-\nMRPC Microsoft Research Paraphrase Corpus tially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\nconsists of sentence pairs automatically extracted\ntasktrainingwithMNLI.\nfromonlinenewssources,withhumanannotations 15https://gluebenchmark.com/faq\njorityclass.\nC AdditionalAblationStudies\nC.1 EffectofNumberofTrainingSteps\nFigure 5 presents MNLI Dev accuracy after fine-\ntuningfromacheckpointthathasbeenpre-trained\nforksteps. Thisallowsustoanswerthefollowing\nquestions:\n1. Question: Does BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhighfine-tuningaccuracy?\nAnswer: Yes, BERT achieves almost\nBASE\n1.0% additional accuracy on MNLI when\ntrainedon1Mstepscomparedto500ksteps.\n2. Question: Does MLM pre-training converge\nslowerthanLTRpre-training,sinceonly15%\nof words are predicted in each batch rather\nthaneveryword?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never,intermsofabsoluteaccuracytheMLM\nmodel begins to outperform the LTR model\nalmostimmediately.\nC.2 AblationforDifferentMasking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixedstrategyformaskingthetargettokenswhen\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n84\n82\n80\n78\n76\n200 400 600 800 1,000\nPre-trainingSteps(Thousands)\nycaruccAveDILNM\nNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nandfine-tuning,asthe[MASK]symbolneverap-\npears during the fine-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both fine-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\nplifiedforthefeature-basedapproachasthemodel\nwill not have the chance to adjust the representa-\ntions.\nMaskingRates DevSetResults\nMASK SAME RND MNLI NER\nFine-tune Fine-tune Feature-based\n80% 10% 10% 84.2 95.4 94.9\n100% 0% 0% 84.3 94.9 94.0\n80% 0% 20% 84.1 95.2 94.6\n80% 20% 0% 84.4 95.2 94.7\n0% 20% 80% 83.7 94.8 94.6\n0% 0% 100% 83.6 94.9 94.6\nTable8: Ablationoverdifferentmaskingstrategies.\nTheresultsarepresentedinTable8. Inthetable,\nMASKmeansthatwereplacethetargettokenwith\nthe[MASK]symbolforMLM; SAME meansthat\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsenttheprobabilitiesofthespecificstrategiesused\nduringMLMpre-training(BERTuses80%,10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures,whichwasshowntobethebestapproach\ninSection5.3.\nFromthetableitcanbeseenthatfine-tuningis\nsurprisinglyrobusttodifferentmaskingstrategies.\nHowever,asexpected,usingonlytheMASKstrat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategyaswell.\nBERTBASE(MaskedLM)\nBERTBASE(Left-to-Right)\nFigure5: Ablationovernumberoftrainingsteps. This\nshows the MNLI accuracy after fine-tuning, starting\nfrom model parameters that have been pre-trained for\nksteps. Thex-axisisthevalueofk.",
  "sections": {
    "abstract": "Abstract There are two existing strategies for apply-\ningpre-trainedlanguage representations todown-\nWe introduce a new language representa-\nstream tasks: feature-based and fine-tuning. The\ntion model called BERT, which stands for\nfeature-based approach, such as ELMo (Peters\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre- et al., 2018a), uses task-specific architectures that\nsentation models (Peters et al., 2018a; Rad- include the pre-trained representations as addi-\nford et al., 2018), BERT is designed to pre- tional features. The fine-tuning approach, such as\ntrain deep bidirectional representations from the Generative Pre-trained Transformer (OpenAI\nunlabeledtextbyjointlyconditioningonboth\nGPT) (Radford et al., 2018), introduces minimal\nleft and right context in all layers. As a re-\ntask-specific parameters, and is trained on the\nsult,thepre-trainedBERTmodelcanbefine-\ndownstream tasks by simply fine-tuning all pre-\ntuned with just one additional output layer\ntrainedparameters. Thetwoapproachessharethe\nto create state-of-the-art models for a wide\nrangeoftasks,suchasquestionansweringand sameobjectivefunctionduringpre-training,where\nlanguage inference, without substantial task- they use unidirectional language models to learn\nspecificarchitecturemodifications. generallanguagerepresentations.\nBERT is conceptually simple and empirically We argue that current techniques restrict the\npowerful. It obtains new state-of-the-art re- power of the pre-trained representations, espe-\nsults on eleven natural language processing cially for the fine-tuning approaches. The ma-\ntasks, including pushing the GLUE score to\njorlimitationisthatstandardlanguagemodelsare\n80.5% (7.7% point absolute improvement),\nunidirectional, and this limits the choice of archi-\nMultiNLI accuracy to 86.7% (4.6% absolute\ntectures that can be used during pre-training. For\nimprovement),SQuADv1.1questionanswer-\ning Test F1 to 93.2 (1.5 point absolute im- example,inOpenAI",
    "introduction": "Introduction\nstrictionsaresub-optimalforsentence-leveltasks,\nLanguage model pre-training has been shown to and could be very harmful when applying fine-\nbe effective for improving many natural language tuningbasedapproachestotoken-leveltaskssuch\nprocessing tasks (Dai and Le, 2015; Peters et al., asquestionanswering,whereitiscrucialtoincor-\n2018a; Radford et al., 2018; Howard and Ruder, poratecontextfrombothdirections.\n2018). Theseincludesentence-leveltaskssuchas In this paper, we improve the fine-tuning based\nnatural language inference (Bowman et al., 2015; approaches by proposing BERT: Bidirectional\nWilliams et al., 2018) and paraphrasing (Dolan Encoder Representations from Transformers.\nand Brockett, 2005), which aim to predict the re- BERT alleviates the previously mentioned unidi-\nlationships between sentences by analyzing them rectionality constraint by using a \u201cmasked lan-\nholistically, as well as token-level tasks such as guage model\u201d (MLM) pre-training objective, in-\nnamedentityrecognitionandquestionanswering, spired by the Cloze task (Taylor, 1953). The\nwheremodelsarerequiredtoproducefine-grained maskedlanguagemodelrandomlymaskssomeof\noutput at the token level (Tjong Kim Sang and the tokens from the input, and the objective is to\nDeMeulder,2003;Rajpurkaretal.,2016). predict the original vocabulary id of the masked\n9102\nyaM\n42\n]LC.sc[\n2v50840.0181:viXra\nword based only on its context. Unlike left-to- These approaches have been generalized to\nright language model pre-training, the MLM ob- coarser granularities, such as sentence embed-\njective enables the representation to fuse the left dings (Kiros et al., 2015; Logeswaran and Lee,\nand the right context, which allows us to pre- 2018)orparagraphembeddings(LeandMikolov,\ntrain a deep bidirectional Transformer. In addi- 2014). To train sentence representations, prior\ntion to the masked language model, we also use work has used objectives to rank candidate next\na \u201cnext sentence prediction\u201d task that jointly pre- s",
    "method": "methods. Pre-trained word embeddings fine-tuned for a supervised downstream task (Dai\nare an integral part of modern NLP systems, of- and Le, 2015; Howard and Ruder, 2018; Radford\nfering significant improvements over embeddings et al., 2018). The advantage of these approaches\nlearnedfromscratch(Turianetal.,2010). Topre- is that few parameters need to be learned from\ntrain word embedding vectors, left-to-right lan- scratch. At least partly due to this advantage,\nguage modeling objectives have been used (Mnih OpenAIGPT(Radfordetal.,2018)achievedpre-\nand Hinton, 2009), as well as objectives to dis- viously state-of-the-art results on many sentence-\ncriminate correct from incorrect words in left and level tasks from the GLUE benchmark (Wang\nrightcontext(Mikolovetal.,2013). et al., 2018a). Left-to-right language model-\nNSP Mask LM Mask LM MNLI NER SQuAD Start/End Span\nC T ... T T T\u2019 ... T \u2019 C T ... T T T\u2019 ... T \u2019\n1 N [SEP] 1 M 1 N [SEP] 1 M\nBERT BERT BERT\nE[CLS] E\n1\n... E\nN\nE\n[SEP]\nE\n1\n\u2019 ... E\nM\n\u2019 E[CLS] E\n1\n... E\nN\nE\n[SEP]\nE\n1\n\u2019 ... E\nM\n\u2019\n[CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM\nMasked Sentence A Masked Sentence B Question Paragraph\nUnlabeled Sentence A and B Pair Question Answer Pair\nPre-training Fine-Tuning\nFigure1: Overallpre-trainingandfine-tuningproceduresforBERT.Apartfromoutputlayers,thesamearchitec-\nturesareusedinbothpre-trainingandfine-tuning. Thesamepre-trainedmodelparametersareusedtoinitialize\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\ning and auto-encoder objectives have been used mal difference between the pre-trained architec-\nfor pre-training such models (Howard and Ruder, tureandthefinaldownstreamarchitecture.\n2018;Radfordetal.,2018;DaiandLe,2015).\nModel Architecture BERT\u2019s model architec-\n2.3 TransferLearningfromSupervised",
    "model": "model called BERT, which stands for\nfeature-based approach, such as ELMo (Peters\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre- et al., 2018a), uses task-specific architectures that\nsentation models (Peters et al., 2018a; Rad- include the pre-trained representations as addi-\nford et al., 2018), BERT is designed to pre- tional features. The fine-tuning approach, such as\ntrain deep bidirectional representations from the Generative Pre-trained Transformer (OpenAI\nunlabeledtextbyjointlyconditioningonboth\nGPT) (Radford et al., 2018), introduces minimal\nleft and right context in all layers. As a re-\ntask-specific parameters, and is trained on the\nsult,thepre-trainedBERTmodelcanbefine-\ndownstream tasks by simply fine-tuning all pre-\ntuned with just one additional output layer\ntrainedparameters. Thetwoapproachessharethe\nto create state-of-the-art models for a wide\nrangeoftasks,suchasquestionansweringand sameobjectivefunctionduringpre-training,where\nlanguage inference, without substantial task- they use unidirectional language models to learn\nspecificarchitecturemodifications. generallanguagerepresentations.\nBERT is conceptually simple and empirically We argue that current techniques restrict the\npowerful. It obtains new state-of-the-art re- power of the pre-trained representations, espe-\nsults on eleven natural language processing cially for the fine-tuning approaches. The ma-\ntasks, including pushing the GLUE score to\njorlimitationisthatstandardlanguagemodelsare\n80.5% (7.7% point absolute improvement),\nunidirectional, and this limits the choice of archi-\nMultiNLI accuracy to 86.7% (4.6% absolute\ntectures that can be used during pre-training. For\nimprovement),SQuADv1.1questionanswer-\ning Test F1 to 93.2 (1.5 point absolute im- example,inOpenAIGPT,theauthorsusealeft-to-\nprovement) and SQuAD v2.0 Test F1 to 83.1 right architecture, where every token can only at-\n(5.1pointabsoluteimprovement). tendtoprevioustokensintheself-attentionlayer",
    "experiments": "experiments, we mask 15% of all WordPiece to-\nWe use WordPiece embeddings (Wu et al., kens in each sequence at random. In contrast to\n2016) with a 30,000 token vocabulary. The first denoisingauto-encoders(Vincentetal.,2008),we\ntoken of every sequence is always a special clas- only predict the masked words rather than recon-\nsification token ([CLS]). The final hidden state structingtheentireinput.\ncorresponding to this token is used as the ag- Although this allows us to obtain a bidirec-\ngregate sequence representation for classification tional pre-trained model, a downside is that we\ntasks. Sentence pairs are packed together into a are creating a mismatch between pre-training and\nsinglesequence. Wedifferentiatethesentencesin fine-tuning,sincethe[MASK]tokendoesnotap-\ntwo ways. First, we separate them with a special pear during fine-tuning. To mitigate this, we do\ntoken([SEP]).Second,weaddalearnedembed- not always replace \u201cmasked\u201d words with the ac-\nding to every token indicating whether it belongs tual [MASK] token. The training data generator\ntosentenceAorsentenceB.AsshowninFigure1, chooses15%ofthetokenpositionsatrandomfor\nwedenoteinputembeddingasE,thefinalhidden prediction. If the i-th token is chosen, we replace\nvector of the special [CLS] token as C \u2208 RH, thei-th token with (1) the[MASK]token 80% of\nand the final hidden vector for the ith input token the time (2) a random token 10% of the time (3)\nasT \u2208 RH. the unchanged i-th token 10% of the time. Then,\ni\nFor a given token, its input representation is T i will be used to predict the original token with\nconstructedbysummingthecorrespondingtoken, cross entropy loss. We compare variations of this\nsegment, and position embeddings. A visualiza- procedureinAppendixC.2.\ntionofthisconstructioncanbeseeninFigure2.\nTask #2: Next Sentence Prediction (NSP)\n3.1 Pre-trainingBERT Many important downstream tasks such as Ques-\ntionAnswering(QA)andNaturalLanguageInfer-\nUnlike Peters et al. (2018a) and Radford et al.\nence (NLI) are",
    "results": "results on many sentence-\ncriminate correct from incorrect words in left and level tasks from the GLUE benchmark (Wang\nrightcontext(Mikolovetal.,2013). et al., 2018a). Left-to-right language model-\nNSP Mask LM Mask LM MNLI NER SQuAD Start/End Span\nC T ... T T T\u2019 ... T \u2019 C T ... T T T\u2019 ... T \u2019\n1 N [SEP] 1 M 1 N [SEP] 1 M\nBERT BERT BERT\nE[CLS] E\n1\n... E\nN\nE\n[SEP]\nE\n1\n\u2019 ... E\nM\n\u2019 E[CLS] E\n1\n... E\nN\nE\n[SEP]\nE\n1\n\u2019 ... E\nM\n\u2019\n[CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM [CLS] Tok 1 ... Tok N [SEP] Tok 1 ... TokM\nMasked Sentence A Masked Sentence B Question Paragraph\nUnlabeled Sentence A and B Pair Question Answer Pair\nPre-training Fine-Tuning\nFigure1: Overallpre-trainingandfine-tuningproceduresforBERT.Apartfromoutputlayers,thesamearchitec-\nturesareusedinbothpre-trainingandfine-tuning. Thesamepre-trainedmodelparametersareusedtoinitialize\nmodels for different down-stream tasks. During fine-tuning, all parameters are fine-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\ning and auto-encoder objectives have been used mal difference between the pre-trained architec-\nfor pre-training such models (Howard and Ruder, tureandthefinaldownstreamarchitecture.\n2018;Radfordetal.,2018;DaiandLe,2015).\nModel Architecture BERT\u2019s model architec-\n2.3 TransferLearningfromSupervisedData tureisamulti-layerbidirectionalTransformeren-\ncoder based on the original implementation de-\nTherehasalsobeenworkshowingeffectivetrans-\nscribed in Vaswani et al. (2017) and released in\nferfromsupervisedtaskswithlargedatasets,such\nthe tensor2tensor library.1 Because the use\nas natural language inference (Conneau et al.,\nofTransformershasbecomecommonandourim-\n2017) and machine translation (McCann et al.,\nplementation is almost identical to the original,\n2017). Computervisionresearchhasalsodemon-\nwe will omit an exhaustive background descrip-\nstrated the importance of transfer learning from\ntionofthemodelarchitecturean"
  }
}