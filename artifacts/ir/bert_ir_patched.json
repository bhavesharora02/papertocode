{
  "paper": {
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "arxiv_id": "1810.04805",
    "tasks": [
      "Language Understanding",
      "Question Answering",
      "Language Inference",
      "Text Classification",
      "Sequence Tagging"
    ],
    "domain": "Natural Language Processing"
  },
  "dataset": {
    "name": "BooksCorpus and English Wikipedia",
    "source": "hf://dummy",
    "subset_fraction": 1.0,
    "splits": {
      "train": 0.8,
      "val": 0.1,
      "test": 0.1
    },
    "features": {
      "text_a": "sentence_A",
      "text_b": "sentence_B",
      "label": "is_next"
    }
  },
  "model": {
    "family": "Transformer",
    "variant": "BERT_LARGE",
    "framework": "torch",
    "layers": [
      {
        "type": "TransformerEncoder",
        "params": {
          "num_hidden_layers": 24,
          "hidden_size": 1024,
          "num_attention_heads": 16,
          "intermediate_size": 4096,
          "hidden_act": "gelu"
        }
      }
    ],
    "init": {
      "pretrained": null
    },
    "num_labels": 2
  },
  "training": {
    "loss": "MaskedLMLoss + NextSentencePredictionLoss",
    "optimizer": {
      "name": "Adam",
      "lr": 0.0001,
      "kwargs": {
        "beta1": 0.9,
        "beta2": 0.999,
        "weight_decay": 0.01
      }
    },
    "scheduler": {
      "name": "linear",
      "kwargs": {
        "warmup_ratio": 0.01
      }
    },
    "batch_size": 256,
    "epochs": 40,
    "metrics": [
      "accuracy",
      "perplexity"
    ],
    "target_metrics": {
      "GLUE Score": 80.5,
      "MultiNLI Accuracy": 86.7,
      "SQuAD v1.1 F1": 93.2,
      "SQuAD v2.0 F1": 83.1
    },
    "tolerance": 0.05
  },
  "preprocessing": {
    "tokenizer": "WordPiece",
    "max_len": 512
  }
}